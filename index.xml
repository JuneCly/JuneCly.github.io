<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chen LY</title>
    <link>https://JuneCly.github.io/</link>
    <description>Recent content on Chen LY</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 24 Nov 2022 22:36:44 +0800</lastBuildDate>
    
        <atom:link href="https://JuneCly.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>METER</title>
      <link>https://JuneCly.github.io/post/meter/</link>
      <pubDate>Thu, 24 Nov 2022 22:36:44 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/meter/</guid>
      
        <description>&lt;p&gt;CVPR 2022，微软发表的文章&lt;em&gt;&lt;strong&gt;An Empirical Study of Training End-to-End Vision and Language Transformers&lt;/strong&gt;&lt;/em&gt;围绕Vision-and-Language Pre-training (VLP)，提出了&lt;strong&gt;METER&lt;/strong&gt; 模型(&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;E&lt;/strong&gt;nd-to-end &lt;strong&gt;T&lt;/strong&gt;ransform&lt;strong&gt;ER&lt;/strong&gt;)。本文偏向综述性质，对现有的VLP模型的框架及预训练任务进行了总结对比，并在大量实验结果的基础上给出了端到端的基于transformer的VLP模型框架：METER。&lt;/p&gt;
&lt;p&gt;（本篇博客也对论文中提到的相关VLP模型进行简单介绍并附上论文链接）&lt;/p&gt;
&lt;p&gt;原文： &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.pdf&#34;&gt;An Empirical Study of Training End-to-End Vision-and-Language Transformers &lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Vision-and-language方向的任务，比如VQA，image-text retrieval都需要模型去理解图像和文本的输入内容。而VLP现在已经成为了用来解决这些问题的常用的方法。&lt;/p&gt;
&lt;p&gt;Transformer在NLP方向一直很流行，最近也在CV领域展示出了很大的潜力。几乎现有的VLP模型都使用Transformer作为模态融合模块的网络结构，使用预训练好的目标检测器从图像中提取区域特征(region feature)。&lt;/p&gt;
&lt;p&gt;但这种使用目标检测器来处理图像的模型架构存在一定的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;目标检测器的效果并非完美，而且会在VLP阶段冻结，这会限制VLP模型的容量；&lt;/li&gt;
&lt;li&gt;目标检测器提取图像中的区域特征十分耗时；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;与此同时，视觉Transformer(ViTs)在处理计算机视觉方向的问题时的效果也展现了很大的前景。&lt;/p&gt;
&lt;p&gt;那么，是否能够训练一个使用ViTs作为图像编码器，整个模型完全由Transformer构成的VLP模型呢？&lt;/p&gt;
&lt;p&gt;最近一些试图使用视觉Transformer的模型并没有取得令人满意的效果，而且通常比SOTA水平的提取图像区域特征的VLP模型效果差。&lt;/p&gt;
&lt;p&gt;为了缩小这两类模型的差距，微软提出了METER(&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;E&lt;/strong&gt;nd-to-end &lt;strong&gt;T&lt;/strong&gt;ransform&lt;strong&gt;ER&lt;/strong&gt;)模型。在构建模型的过程中，微软详细调研了应该如何训练出一个完全由Transformer构成的端到端的VLP模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic01.png&#34; alt=&#34;An overview of the proposed METER framework&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，微软从不同的模型设计角度进行实验对比，对比方向如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;vision encoders&lt;/strong&gt; (e.g., CLIP-ViT, Swin transformer)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;text encoders&lt;/strong&gt; (e.g., RoBERTa, DeBERTa)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;multimodal fusion module&lt;/strong&gt; (e.g., merged attention vs. co-attention)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;architectural design&lt;/strong&gt; (e.g., encoder-only vs. encoder-decoder)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pre-training objectives&lt;/strong&gt; (e.g., masked image modeling)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;经过大量的实验和测试，可以得出以下结论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vision transformer (ViT) 在VLP中发挥的作用比language transformer大，而且transformer的在纯图像或纯文本任务上的表现并不能代表其在VL任务上的表现；&lt;/li&gt;
&lt;li&gt;使用交叉注意力 (cross-attention) 有利于多模态融合，在下游任务中的表现也比单独使用自注意力机制的效果好；&lt;/li&gt;
&lt;li&gt;对于VQA任务和零样本图文检索 (zero-shot image-text retrieval) 任务，只用Encoder的VLP模型要比使用Encoder-Decoder结构的VLP模型效果好；&lt;/li&gt;
&lt;li&gt;在VL模型预训练时，添加masked image modeling损失函数并不会提高下游任务的表现效果。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;根据这些建模技巧，本篇工作提出的METER模型，在VQAv2测试集上达到77.64 %的准确率，超过了先前最好的基于区域特征的VinVL模型1.04 %，超过了先前最好的基于ViT的模型 (ALBEF) 1.6%。&lt;/p&gt;
&lt;h2 id=&#34;2-glossary-of-vlp-models&#34;&gt;2. Glossary of VLP Models&lt;/h2&gt;
&lt;p&gt;本部分对VLP模型进行概述，并依据其对图像编码方式的不同将VLP模型分为三类&lt;/p&gt;
&lt;p&gt;对VLP模型结构的特点总结如下表：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic02.png&#34; alt=&#34;Glossary of representative VLP models&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;od-based-reigon-features&#34;&gt;OD-based Reigon Features&lt;/h3&gt;
&lt;p&gt;之前的大多数模型都使用预训练好的目标检测器(pre-trained object detectors)来提取图像特征。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在这些模型之中，有的模型（如ViLBERT, LXMERT）用两个transformer分别处理图像和文本信息，再使用另一个co-attention transformer在后期对两种模态进行融合。&lt;/p&gt;
&lt;p&gt;还有的模型（如Visual-BERT, VL-BERT, UNITER）将图像和文本信息一起送入一个transformer，实现模态融合。&lt;/p&gt;
&lt;p&gt;OSCAR和VinVL将额外的图像标签送入transformer中，达到了VL任务的SOTA水平。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但是，提取区域特征十分耗时，预训练好的目标检测器通常在预训练的时候参数固定，这会限制VLP模型的效果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf&#34;&gt;ViLBERT&lt;/a&gt; （NeurIPS 2019）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型简介&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;以BERT模型作为基础，将模型拓展为&lt;strong&gt;多模态双流模型&lt;/strong&gt;(multi-modal two-stream model)，分别处理视觉和文本信息的输入，通过co-attention的transformer层进行交互。在大规模的自动收集的概念描述的数据集上通过两个代理任务来进行模型的预训练，然后仅对基础模型做微小的改进后，将模型迁移到众多VL任务上，如VQA，视觉常识推理(visual commonsense reasoning)，指称表达(referring expression) 和基于描述的图像检索(caption-based image-retrieval)。实验结果表明ViLBERT模型比单独面向各个任务的特定模型的效果都更好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型结构图&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;下面分别是ViLBERT模型结构图和模型中使用的co-attention的transformer层结构图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/ViLBERT01.png&#34; alt=&#34;ViLBERT model&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/ViLBERT02.png&#34; alt=&#34;ViLBERT co-attention transformer&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.07490&#34;&gt;LXMERT&lt;/a&gt;  （EMNLP 2019）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型简介：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;视觉语言推理任务需要模型学习视觉概念、文本语义以及两种模态之间的对齐和理解。LXMERT (Learning Cross-Modality Encoder Representations from Transformers) 模型就可以学习这些视觉-语言之间的关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型结构图：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/LXMERT01.png&#34; alt=&#34;LXMERT model&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.08530&#34;&gt;VL-BERT&lt;/a&gt;  （ICLR 2020）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型简介：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;VL-BERT在BERT基础上进行改进，将文本信息和RoI图像信息送入一个BERT中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型结构图：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/VL-BERT01.png&#34; alt=&#34;LXMERT model&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.06165&#34;&gt;OSCAR&lt;/a&gt;  （ECCV 2020）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OSCAR提出一种新的VLP方法：可以将VL模型的输入变成一个三元组形式，分别由Word Tokens，Object Tags和Region Features构成。可以帮助模型快速掌握文本和图像之间的语义对齐。本篇工作还提出了OSCAR模型，在多个VL任务上达到了新的SOTA。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/OSCAR01.png&#34; alt=&#34;OSCAR预训练范式&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/OSCAR02.png&#34; alt=&#34;OSCAR预训练模型&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cnn-based-grid-features&#34;&gt;CNN-based Grid Features&lt;/h3&gt;
&lt;p&gt;为了解决上面提到的两个问题，人们一直在尝试使用不同的方法来实现端到端的预训练VL模型。&lt;/p&gt;
&lt;p&gt;Pixel-BERT和CLIP-ViL将从CNN中提取的网格特征和文本信息直接送入Transformer中训练。&lt;/p&gt;
&lt;p&gt;SOHO提出先根据视觉词典(vision dictionary)将网格特征离散化，然后再将这些离散化特征送入跨模态模块。虽然直接使用网格特征高效，但不一致的优化器通常用于CNN和transformer。&lt;/p&gt;
&lt;p&gt;比如，Pixel-BERT和CLIP-ViL对transformer部分使用AdamW优化器，CNN部分使用SGD。最近关于vision transformers的工作表明，使用CNN比使用ViT的VLP模型的精度要略差一些，这也促使人们开发基于ViT的VLP模型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.00849&#34;&gt;Pixel-BERT&lt;/a&gt;  （arXiv 2020）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型简介：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用图像-句子对作为输入，输出每个输入元素的注意力特征。图像送入一个像素级特征embedding模块逐个像素运算，句子则被送入一个句子特征embedding模块逐个token运算。预训练方式使用MLM和ITM(Image-Text Matching)任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型结构图：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/Pixel-BERT01.png&#34; alt=&#34;ViLBERT model&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.06383&#34;&gt;CLIP-ViL&lt;/a&gt;  （ICLR 2022）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型简介：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将CLIP直接应用于特定的任务的模型，称为CLIP-ViL，并对三个任务（VQA，Image Captioning，Vision-and-Language Navigation）进行微调。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vit-based-patch-features&#34;&gt;ViT-based Patch Features&lt;/h3&gt;
&lt;p&gt;ViLT直接将图像的patch features和文本token embeddings送入预训练的ViT模型中，然后在图像字幕的数据集上进行微调。但这些模型在诸如VQA的下游任务上的表现都逊于SOTA水平。&lt;/p&gt;
&lt;p&gt;在本文中，主要研究如何以端到端的方式预训练一个基于ViT的模型，让其在保持快速推理的同时缩小性能上的差距。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://proceedings.mlr.press/v139/kim21k.html&#34;&gt;ViLT&lt;/a&gt; （ICML 2021）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型简介：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ViLT将Visual Embedding模块和Text Embedding都设计成轻量模块，将主要的计算都放在模态交互部分。ViLT首先直接使用预训练的ViT来初始化交互的Transformer，这样就可以直接利用交互层来处理图像特征。对于视觉模态，将图像分割成图像块，通过Linear Projection转化成Patch position embedding。对于文本模态，直接使用word embedding将文本转换成Token position embedding。两个模态得到的embedding分别和modal-type embedding进行concate，再和额外的[class] embedding进行concate，并于对接下游任务。将得到的embedding输入Transformer Encoder中。&lt;/p&gt;
&lt;p&gt;预训练过程中使用了两个优化目标：ITM(image text matching) 和MLM(masked language embedding)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型结构图：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/ViLT01.png&#34; alt=&#34;ViLT model&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Embedding Schema&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;VLP模型中textual embedder大多是都是BERT或者BERT的改进形式，但是visual embedders却不尽相同。绝大多是情况下，visual embedding是现有VLP模型的瓶颈部分。现有的visual embedding多数是提取&lt;strong&gt;region features&lt;/strong&gt;或者&lt;strong&gt;grid features&lt;/strong&gt;。ViLT模型使用&lt;strong&gt;Patch Projection&lt;/strong&gt;（introduced by ViT）来减小计算开销，以便实现轻量目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VL模型的分类：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;注：下图中各长方形的高代表了相关计算量大小。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/ViLT02.png&#34; alt=&#34;four categories of vision-and-language models&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-the-meter-framework&#34;&gt;3. The METER Framework&lt;/h2&gt;
&lt;p&gt;根据之前的调研工作，我们定义了VLP模型中一些比较重要的模块，如下图。在这小节，我们会详细介绍METER的框架结构，以及相关默认设置。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic01.png&#34; alt=&#34;An overview of the proposed METER framework&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;METER framework overview:&lt;/strong&gt;（参考上图）&lt;/p&gt;
&lt;p&gt;给出文本语句l和图像v，VLP模型首先通过vision encoder和text encoder获取图像和文本的特征。然后将这些特征输入到多模态融合模块以生成跨模态的表示，接下来可以选择是否将其输入到decoder中或者直接生成最终想要的输出结果。&lt;/p&gt;
&lt;h3 id=&#34;31-model-architecture&#34;&gt;3.1 Model Architecture&lt;/h3&gt;
&lt;h4 id=&#34;vision-encoder&#34;&gt;Vision Encoder&lt;/h4&gt;
&lt;p&gt;在本文中，我们关注patch features以及探索将ViTs作为vision encoder的情况。ViT中，首先将图像先分割成patch，在将patches送入transformer中。但是，使用了ViTs的VLP模型并没有达到region-feature-based的VLP模型的SOTA水平。因此本文对系统性地对不同的ViTs进行实验，看哪一种更加适合VLP模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文中进行实验对比的ViT模型有：original ViT, DeiT, Distilled-DeiT, CaiT, VOLO, BeiT, Swin Transformer和CLIP-ViT.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;text-encoder&#34;&gt;Text Encoder&lt;/h4&gt;
&lt;p&gt;按照BERT和RoBERTa的做法，VLP模型通常将输入语句拆分成一系列的子词 (subwords)，然后再句首和句尾插入两个特别的token，得到输入文本序列。在得到text embedding之后，要么直接将其送入模态融合模块，或者在融合前先将其送入专门处理文本的网络层中。对于前者，因为文本编码和多模态融合的角色融合在一起，所以通常使用一个单独的BERT模型。但在本文中，我们希望将这一模块拆分成两个部分，先用一个text encoder对文本进行编码，再将得到的结果送入模态融合模块。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;虽然有很多不同的预训练LM种类，但大多数VLP模型仍然使用BERT来实现文本信息的初始化。在本文中，作者将BERT, RoBERTa, ELECTRA, ALBERT和DeBERTa作为text encoder进行实验。除此之外，也对由BERT初始化的word embedding look-up layer进行实验。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;multimodal-fusion&#34;&gt;Multimodal Fusion&lt;/h4&gt;
&lt;p&gt;对两种模态融合方式进行研究，分别是merged attention和co-attention（如下图）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic03.png&#34; alt=&#34;merged attention &amp;amp; co-attention&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于merged attention方式，只需将文本和图像信息拼接在一起，就可以送入一个transformer中进行训练。这种方式的优点是参数的高效性，对于两种模态使用一套参数即可。&lt;/p&gt;
&lt;p&gt;对于co-attention方式，需要先将文本和图像分别送入不同的transformer模块进行训练，然后再利用co-attention的方式让不同模态之间的信息实现交互。&lt;/p&gt;
&lt;p&gt;在reigon-based的VLP模型中，两种模态融合的方式实现的效果差不多，但随着端到端的VLP模型热度逐渐升高，这篇工作对两种模态融合方式得到的效果重新实验。&lt;/p&gt;
&lt;h4 id=&#34;encoder-only-vs-encoder-decoder&#34;&gt;Encoder-Only vs. Encoder-Decoder&lt;/h4&gt;
&lt;p&gt;许多VLP模型会使用encoder-only结构，将得到的跨模态表示直接送入输出层生成最终结果。但最近有一些模型（如：VL-T5, SimVLM）提倡使用encoder-decoder结构，即先将跨模态表示先送入decoder再送入输出层。在这种模型架构中，decoder既关注来自于encoder的representation，也关注之前生成的tokens。&lt;/p&gt;
&lt;p&gt;下图展示这两种结构在实现预训练MLM任务时的不同之处。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic04.png&#34; alt=&#34;encoder-only &amp;amp; encoder-decoder&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-pre-training-objectives&#34;&gt;3.2 Pre-training Objectives&lt;/h3&gt;
&lt;p&gt;本部分先介绍广泛应用于VLP模型的MLM和ITM预训练任务，再介绍如何探索设计MIM任务。&lt;/p&gt;
&lt;h4 id=&#34;masked-language-modeling&#34;&gt;Masked Language Modeling&lt;/h4&gt;
&lt;p&gt;MLM任务一开始是应用在纯文本的预训练中的，但其在VLP中的有效性也得到认可。对于image-caption pair，我们随机遮盖一些输入token，训练模型根据被遮盖的字符以及相应的图像输入去还原文本。&lt;/p&gt;
&lt;h4 id=&#34;image-text-matching&#34;&gt;Image-Text Matching&lt;/h4&gt;
&lt;p&gt;在ITM任务中，模型需要在一系列image-caption pairs中判别哪些是匹配的，哪些不是。大多数VLP模型都将ITM看作一个二分类任务。&lt;/p&gt;
&lt;h4 id=&#34;masked-image-modeling&#34;&gt;Masked Image Modeling&lt;/h4&gt;
&lt;p&gt;类比MLM，研究人员尝试在图像端使用masked image modeling。比如，在之前的一些工作中（如LXMERT, UNITER）遮盖一些输入图像区域，然后训练模型去预测region feature，属于回归任务。也可以先用能够获得高层的语义信息的预训练目标检测器对每个区域生成一个对象标签，然后训练模型根据被遮盖的区域去直接预测标签，而不是预测region feature。&lt;/p&gt;
&lt;p&gt;但最近一些SOTA的VLP模型（ALBEF, VinVL）没有使用MIM预训练任务。在ViLT这篇工作中，作者也提出masked patch regression对他们的设置并没有起到帮助。MIM对于VLP模型是否有效并不确定。为了进一步研究MIM的作用，我们将MIM看作一个masked patch classification任务，并提出两种实现方式。&lt;/p&gt;
&lt;h5 id=&#34;1-masked-patch-classification-with-in-batch-negatives&#34;&gt;1) Masked Patch Classification with In-batch Negatives&lt;/h5&gt;
&lt;p&gt;构建候选集，预测时直接从候选集中选择。&lt;/p&gt;
&lt;h5 id=&#34;2-masked-patch-classification-with-discrete-code&#34;&gt;2) Masked Patch Classification with Discrete Code&lt;/h5&gt;
&lt;p&gt;受BEiT启发，获取输入patches的离散表示，然后训练模型去重构这些离散tokens。&lt;/p&gt;
&lt;h3 id=&#34;33-our-default-settings-for-meter&#34;&gt;3.3 Our Default Settings for METER&lt;/h3&gt;
&lt;p&gt;METER有很多不同的设计方式，在此部分介绍默认设置。&lt;/p&gt;
&lt;h4 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h4&gt;
&lt;p&gt;使用co-attention方式实现模态融合。底部用一个预训练的visual encoder和一个text encoder。在每个encoder上叠加6个transformer编码层，每层都有一个自注意力模块，一个co-attention模块和一个前馈神经网络模块组成。模型中不包括decoder，图像和语言分支的参数也不共享。&lt;/p&gt;
&lt;h4 id=&#34;pre-training-objectives&#34;&gt;Pre-training Objectives&lt;/h4&gt;
&lt;p&gt;用MLM和ITM作为预训练任务。&lt;/p&gt;
&lt;h4 id=&#34;pre-training-datasets&#34;&gt;Pre-training Datasets&lt;/h4&gt;
&lt;p&gt;在四个最常用的数据集上进行预训练。数据集分别是：COCO, Conceptual Captions,  SBU Captions, Visual Genome，一共约有4M图片。&lt;/p&gt;
&lt;h4 id=&#34;downstream-tasks&#34;&gt;Downstream Tasks&lt;/h4&gt;
&lt;p&gt;对于消融实验和分析部分，我们主要使用VQAv2进行实验。为了对比SOTA水平，也对其他任务进行评估，如visual reasoning(NLVR2)，visual entailment(SNLI-VE)，image-text retrieval(COO, Flickr30k)。&lt;/p&gt;
&lt;h2 id=&#34;4-experiments&#34;&gt;4. Experiments&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;论文本部分内容主要是实验对比，故在此主要给出实验结果和相关结论。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;41-impact-of-vision-and-language-encoders&#34;&gt;4.1 Impact of Vision and Language Encoders&lt;/h3&gt;
&lt;h4 id=&#34;without-vlp&#34;&gt;without VLP&lt;/h4&gt;
&lt;p&gt;省去VLP预训练过程，底层使用预训练text encoder和vision encoder，顶层随机初始化后直接在具体的下游任务上进行微调测试。实验对比结果如下表所示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic05.png&#34; alt=&#34;vision and text encoders without pre-training&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：如果不进行VLP预训练，直接在下游任务上进行微调的话，text encoder中RoBERTa和Swin Transformer效果好，visual encoder中CLIP-ViT效果好。&lt;/p&gt;
&lt;h4 id=&#34;with-vlp&#34;&gt;with VLP&lt;/h4&gt;
&lt;p&gt;正如下表所示，加上VLP之后，text encoder方面BERT和RoBERTa之间的差距似乎消失了，但底层使用一个预训练好的text encoder还是十分必要的（对比Embed-only和BERT）。对于vision encoder来说，CLIP-ViT-224/16和Swin Transformer都能达到比较好的水平。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic06.png&#34; alt=&#34;different encoders&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Useful Tricks:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机初始化的参数使用的学习率可以比由预训练模型初始化的参数使用的学习率大一些。&lt;/li&gt;
&lt;li&gt;在微调过程中提高图片的分辨率，得到的效果更好。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;42-analysis-of-the-multimodal-fusion-module&#34;&gt;4.2 Analysis of the Multimodal Fusion Module&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：对比co-attention和merged attention，发现co-attention的效果更好一些，说明对于不同模态训练不同的参数是十分必要的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic07.png&#34; alt=&#34;co-att vs merged-att&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;43-encoder-only-vs-encoder-decoder&#34;&gt;4.3 Encoder-Only vs Encoder-Decoder&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：根据4.2小节的表可以看出，encoder-only模型的效果比encoder-decoder的效果好。但是encoder-decoder模型更灵活，更适合处理image captioning等任务。&lt;/p&gt;
&lt;h3 id=&#34;44-ablations-on-pre-training-objectives&#34;&gt;4.4 Ablations on Pre-training Objectives&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：根据下表，MLM和ITM都可以提升下游任务的效果。但是本篇工作中提出的两种MIM预训练任务会让实验结果变差。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic08.png&#34; alt=&#34;pre-training objectives&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;45-comparison-with-prior-arts&#34;&gt;4.5 Comparison with Prior Arts&lt;/h3&gt;
&lt;p&gt;对METER最好的模型架构进行测试评估，并与之前的工作进行对比，结果如下表。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;best-performing models:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RoBERTa-base+Swin Transformer and RoBERTbase+CLIP-ViT-224/16 with co-attention fusion module, and with image resolutions set to 384 and 288, respectively&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic09.png&#34; alt=&#34;evaluaion&#34;&gt;&lt;/p&gt;
&lt;p&gt;除此之外，METER模型还具有可拓展性。如下表所示，模型可以在VQAv2上达到SOTA水平，超过了之前使用1.8B图像训练的模型的结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/METERPic/METERPic10.png&#34; alt=&#34;Scaling the Model&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h2&gt;
&lt;p&gt;本篇工作提出了METER模型架构，基于此对如何训练一个端到端的全部由transformer构成的VLP模型进行了系统全面的实验。实验结果表明，可以用4M图像对模型进行预训练，得到的效果可以和SOTA水平的模型相媲美。进一步扩大模型规模时，METER可以实现新的SOTA水平。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Prompt（一）: 基础知识</title>
      <link>https://JuneCly.github.io/post/prompt-01/</link>
      <pubDate>Fri, 18 Nov 2022 22:22:29 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/prompt-01/</guid>
      
        <description>&lt;p&gt;prompt是NLP领域最新兴起的预训练范式，其主要方法是在预训练的过程中结合输入文本给出与下游任务相关的提示，让模型在训练过程中进行预测。本篇博客主要介绍NLP领域语言模型的四个发展阶段、prompt基本形式的数学定义以及相关的背景知识。&lt;/p&gt;
&lt;h2 id=&#34;1-background-knowledge&#34;&gt;1. Background Knowledge&lt;/h2&gt;
&lt;p&gt;本部分内容都可以分别展开详述，此处仅作简单概念介绍。&lt;/p&gt;
&lt;h3 id=&#34;zero-shot-learning&#34;&gt;Zero-shot Learning&lt;/h3&gt;
&lt;p&gt;Zero-shot Learning(ZSL)，又称零样本学习。ZSL的目标是让模型具有推断能力，能够根据一些具体的描述分类出在训练中不曾学习过的类别对象（Zero-shot的来源）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ShotLearningPic/zeroshot01.png&#34; alt=&#34;Zeor-shot Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;举个例子，假设模型通过训练可以识别出马、老虎以及熊猫这三种类别的动物。并且能够从中抽取出不同类别的特征，比如：马的外形特征，老虎的条纹状特征以及熊猫的黑白色特征。此时当我们给出对于斑马的描述（外形像马、黑白色、条纹）时，我们希望模型能够识别出斑马。&lt;/p&gt;
&lt;h3 id=&#34;few-shot-learning&#34;&gt;Few-shot Learning&lt;/h3&gt;
&lt;p&gt;Few-shot Learning(FSL)，又称少样本学习。意思是面对一个新的类别，模型只需要少量的样本就能够学习。Few-shot Learning是Meta Learning（元学习）在监督学习领域的应用。&lt;/p&gt;
&lt;p&gt;该领域的一个常用术语叫做 &lt;strong&gt;N-way K-shot&lt;/strong&gt;，表示在训练阶段，使用N个类别，每个类别K个样本的数据作为训练数据，让模型从小样本的数据中学习如果对这N个类别进行分类。&lt;/p&gt;
&lt;h3 id=&#34;one-shot-learning&#34;&gt;One-shot Learning&lt;/h3&gt;
&lt;p&gt;可以看作一种特殊情况下的Few-shot Learning，此时训练阶段不同类别的样本只有一个。&lt;/p&gt;
&lt;h2 id=&#34;2-two-sea-changes-in-nlp&#34;&gt;2. Two Sea Changes in NLP&lt;/h2&gt;
&lt;h3 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h3&gt;
&lt;p&gt;早期的NLP模型依赖于&amp;quot;Feature Engineering&amp;quot;，工程师利用他们了解的领域内的知识从有限的数据中提取特征。&lt;/p&gt;
&lt;h3 id=&#34;architecture-engineering&#34;&gt;Architecture Engineering&lt;/h3&gt;
&lt;p&gt;神经网络面世之后，NLP领域的模型转向了&amp;quot;Architecture Engineering&amp;quot;。在这个阶段，模型自己学习特征&lt;/p&gt;
&lt;h3 id=&#34;objective-engineering&#34;&gt;Objective Engineering&lt;/h3&gt;
&lt;p&gt;从2017年至2019年，NLP领域迎来了一次巨变。有监督模型的重要性逐渐降低。这时期，人们将注意力转移到“预训练-微调”的模型范式上。在这种范式中，有着固定结构的模型作为语言模型被提前预训练好。这些语言模型可以在海量的数据集上进行预训练，在这个过程中模型就可以学习到这个语言相关的特征信息。经过预训练的语言模型接下来会通过微调应用于不同的下游任务。在这种范式中，人们的注意力主要在于&amp;quot;Objective Engineering&amp;quot;，即设计能应用在预训练阶段和微调阶段的训练目标。&lt;/p&gt;
&lt;h3 id=&#34;prompt-engineering&#34;&gt;Prompt Engineering&lt;/h3&gt;
&lt;p&gt;现在我们正处于NLP领域的第二次巨变的过程中，我们正由“预训练-微调”的范式过渡到“预训练-提示-预测”（pre-train, prompt, predict）范式。不同于之前通过objective engineering让预训练模型去适应不同的下游任务，prompt的重点在于借助“提示”的方法让下游任务可以直接在模型预训练的阶段进行训练。&lt;/p&gt;
&lt;p&gt;比如说对于情感分析任务，对于输入语句”我今天错过了公交车“，我们可以在后面加上”我感觉十分___“的提示语句，让模型用表达情感的词来填空。通过这种方法，我们可以操控预训练模型去预测我们想要的输出，有时甚至不需要额外的和具体任务相关的训练。&lt;/p&gt;
&lt;p&gt;prompt的优点在于：当我们给出合适的提示后，我们可以用一个无监督方式训练的语言模型去解决很多任务。&lt;/p&gt;
&lt;p&gt;这种范式应用的关键之处在于&amp;quot;Prompt Emgineering&amp;quot;，即为当前的语言模型找一个最合适的prompt。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/PromptPic/PromptPic01.png&#34; alt=&#34;Four paradigms in NLP&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-a-formal-description-of-prompting&#34;&gt;3. A Formal Description of Prompting&lt;/h2&gt;
&lt;h3 id=&#34;supervised-learning-in-nlp&#34;&gt;Supervised Learning in NLP&lt;/h3&gt;
&lt;p&gt;NLP的传统监督学习模式下，我们通常选择文本作为输入x，通过模型P (y|x; θ)预测输出y。y可以是一个标签、文本或是其他形式的输出。为了学习到模型的中的参数，我们使用包含输入-输出对的数据集来训练模型，去预测条件概率。&lt;/p&gt;
&lt;p&gt;举例如下：&lt;/p&gt;
&lt;p&gt;1、文本分类：选用文本作为输入x，从固定的标签集中选择一个标签作为输出y。如情感分析。&lt;/p&gt;
&lt;p&gt;2、条件文本生成 conditional text generation：通过输入文本x生成另一个文本y。如机器翻译。&lt;/p&gt;
&lt;h3 id=&#34;prompting-basics&#34;&gt;Prompting Basics&lt;/h3&gt;
&lt;p&gt;有监督学习方式的主要问题在于，为了训练一个模型需要大量的带标签的数据集。但是对于许多任务来说，这样大量的数据集很难找到。NLP方向的基于提示的学习方法试图通过学习语言模型来避开这个问题。语言模型根据输入文本x直接学习概率，然后根据这个概率去预测y，以此来减少或避免对于大型的带标签的数据集的需求。&lt;/p&gt;
&lt;p&gt;本节给出提示的最基础的数学描述，既包含关于提示的许多工作，也可以扩展到其他的工作。&lt;/p&gt;
&lt;p&gt;基础的提示(Basic Prompting)通过&lt;strong&gt;三个步骤&lt;/strong&gt;预测最高得分的输出y&lt;/p&gt;
&lt;h4 id=&#34;step-1-prompt-addition-添加提示&#34;&gt;Step 1: Prompt Addition 添加提示&lt;/h4&gt;
&lt;p&gt;需要一个提示函数fprompt(·)将输入x变为提示x’，在之前的大多数工作中，这一函数通常分为两步：&lt;/p&gt;
&lt;p&gt;1、使用一段包含两处空缺的文本作为模板。一个input slot用于输入[X]，一个answer slot用于输出中间生成答案[Z]，文本z将在后面被映射为y。&lt;/p&gt;
&lt;p&gt;2、将输入x填入slot[X]处。&lt;/p&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;prompt分类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cloze prompt&lt;/strong&gt;: 将slot放在文本中间的位置。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;prefix prompt&lt;/strong&gt;: 将slot放在文本的末尾处。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;prompt不一定要是文字，也可以是虚拟文字（如数字id），或直接生成连续的向量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 和[Z]的slots数量可以根据任务的需要灵活改变。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/PromptPic/PromptPic02.png&#34; alt=&#34;Terminology and notation of prompting methods&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;step-2-answer-search&#34;&gt;Step 2: Answer Search&lt;/h4&gt;
&lt;p&gt;这一步的目标是寻找一个是让LM得分最高的文本z。我们设定Z集合作为z的答案的集合。Z可以是自然语言的集合（适用于语言生成任务），也可以是固定词的集合（适用于文本分类任务）。&lt;/p&gt;
&lt;p&gt;我们接着定义一个函数 ffill(x′, z)，对于一个prompt x填入一个潜在答案z。我们把进行过这一操作的prompt称为filled prompt。如果提示里填写的是正确答案，我们称其为answered prompt&lt;/p&gt;
&lt;p&gt;最后，我们用预训练的语言模型在Z集合内的潜在答案中计算求对应的filled prompt的概率。&lt;/p&gt;
&lt;h4 id=&#34;step-3-answer-mapping-答案映射&#34;&gt;Step 3: Answer Mapping 答案映射&lt;/h4&gt;
&lt;p&gt;最后，我们要根据得分最高的答案z得到得分最高输出结果y。这一映射在大多数的情况下都是繁琐的，因为在一些场景下答案本身就是最后的输出结果。但对于情感分析这种文本分类任务来说，可能不同的z对应的输出y是相同的。所以需要有答案映射这一步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/PromptPic/PromptPic03.png&#34; alt=&#34;Examples of input, template, and answer for different tasks&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;design-considerations-for-prompting&#34;&gt;Design Considerations for Prompting&lt;/h3&gt;
&lt;p&gt;简单介绍一些设计提示是需要考虑的方面，更具体的细节在后续文章详细介绍：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-trained Model Choice&lt;/strong&gt;: 许多预训练模型都可以同来计算概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: 不同的prompt不仅对准确率有很大的影响，同样也有各自适合的不同任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Answer Engineering&lt;/strong&gt;: 根据任务的不同，我们构建的Z集合以及相应的映射函数也不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expanding the Paradigm&lt;/strong&gt;: 上面提到的公式仅用来描述prompt中最基础一些模型架构。对于这个范式还有很多拓展的方法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt-based Training Strategies&lt;/strong&gt;: 对于语言模型的参数，以及prompt的参数以及两者结合的参数的训练都有很多方法。在第7节会对不同的训练策略以及他们相应的优点进行总结。&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title>Self Attention</title>
      <link>https://JuneCly.github.io/post/self-attention/</link>
      <pubDate>Fri, 28 Oct 2022 23:56:33 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/self-attention/</guid>
      
        <description>&lt;p&gt;对于Transformer中Self-attention机制的理解&lt;/p&gt;
&lt;h2 id=&#34;attention-mechanism&#34;&gt;Attention Mechanism&lt;/h2&gt;
&lt;h3 id=&#34;attention机制理解&#34;&gt;attention机制理解&lt;/h3&gt;
&lt;p&gt;将人的感知方式，注意力的行为应用在机器上，让机器学会感知数据中重要的和不重要的部分。&lt;/p&gt;
&lt;h3 id=&#34;attention分类&#34;&gt;attention分类&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Soft/Global Attention（软注意/全局注意）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对每个输入项分配的权重在0-1之间，也就是某些部分关注的多一点，某些部分关注的少一点，因为对大部分信息都关注，但关注程度不同。所以计算量相对来说较大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hard/Local Attention （硬注意/局部注意）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对每个输入项分配的权重非0即1，只关注需要的部分，舍弃掉无关的部分。好处是可以减少一定的时间和计算成本，缺点是有可能丢失信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self/Intra Attention （自注意力机制/内注意）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对每个输入项分配的权重取决于输入项之间的相互作用，即通过输入项内部的“表决”来决定应该关注哪些输入项。与前两者相比，自注意力机制在处理很长的输入时，具有并行计算的优势。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention&#34;&gt;Self-Attention&lt;/h2&gt;
&lt;h3 id=&#34;self-attention的挑战与目标&#34;&gt;Self-Attention的挑战与目标&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The animal didn’t cross the street because it was too tired&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The animal didn’t cross the street because it was too wide&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;两个句子中的it指代的对象并不一样，但是这对于机器来说很难判断。&lt;/p&gt;
&lt;p&gt;self-attention机制通过计算单词it与其他词之间的联系得知it的具体指代。&lt;/p&gt;
&lt;h3 id=&#34;self-attention的计算过程概述&#34;&gt;Self-Attention的计算过程概述&lt;/h3&gt;
&lt;p&gt;对于一系列输入向量a，经过self-attention 后得到一系列向量b，输出的每个token是考虑了输入的所有token后得到的。所以4个向量token对应4个向量的token&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于每个向量a，分别乘上三个矩阵系数W得到三个值：Q, K, V，分别表示query, key, value&lt;/li&gt;
&lt;li&gt;利用得到的Q和K计算每两个输入向量之间的相关性，也就是计算attention的a，常用点乘方法计算。A= K·Q，矩阵A中的每一个值记录了对应的两个输入向量的Attention大小&lt;/li&gt;
&lt;li&gt;对A矩阵进行softmax或者Relu得到A&#39;&lt;/li&gt;
&lt;li&gt;利用得到的A&amp;rsquo;和V计算得到每个输入向量a对应的self-attention层的输出向量b&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;attention核心公式的理解&#34;&gt;Attention核心公式的理解&lt;/h3&gt;
&lt;p&gt;$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$&lt;/p&gt;
&lt;h4 id=&#34;1对公式计算步骤的理解&#34;&gt;1、对公式计算步骤的理解&lt;/h4&gt;
&lt;p&gt;抛开Q，K，V三个矩阵，self-attention的最原始形态的公式如下：
$$
Softmax(XX^T)X
$$
其中
$$
XX^T代表什么
$$
一个矩阵是由一些向量组成的，当一个矩阵乘以自己的转置矩阵时，（将矩阵中的向量以多个行向量组成的方式表示）可以看作这些向量分别与其他向量计算内积。&lt;/p&gt;
&lt;p&gt;而向量内积的几何意义是：表征两个向量的家教，即表征一个向量在另一个向量上的投影。投影的值越大，说明两个向量的相关度越高。&lt;/p&gt;
&lt;p&gt;如果两个向量的夹角是90°，那么这两个向量线性无关，完全没有相关性。&lt;/p&gt;
&lt;p&gt;将词向量代入其中进行理解，若两个词向量计算内积得到的投影的值越大，可以理解为两个词向量的相关性就越高。&lt;/p&gt;
&lt;p&gt;接下来，进一步理解
$$
Softmax(XX^T)中softmax的意义
$$
softmax的意义是进行归一化。而Attention 机制的核心是正是&lt;strong&gt;加权求和&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;接下来我们再看这个公式
$$
Softmax(XX^T)X
$$
我们取经过内积与softmax运算后的一个行向量举例，将其与X的一个列向量相乘，新得到的向量与原来X的向量维度相同。&lt;/p&gt;
&lt;p&gt;在新的向量中，每一个维度的数值都是由三个词向量在这一维度的数值加权求和得到的，这个新的行向量就是原来的“早”字向量经过注意力机制加权求和之后的表示。&lt;/p&gt;
&lt;h4 id=&#34;2对qkv矩阵的理解&#34;&gt;2、对Q、K、V矩阵的理解&lt;/h4&gt;
&lt;p&gt;Q、K、V矩阵都是矩阵X与权重矩阵W的乘积得到的，即他们&lt;strong&gt;本质上都是X的线性变换&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为什么不直接用矩阵X进行计算呢？是为了提升模型的拟合能力。因为矩阵W是可以训练的，可以起到一个缓冲的作用。&lt;/p&gt;
&lt;h4 id=&#34;3对根号d_k的理解&#34;&gt;3、对根号d_k的理解&lt;/h4&gt;
&lt;p&gt;$$
\sqrt{d_k}
$$&lt;/p&gt;
&lt;p&gt;假设Q，K里的元素均值为0，方差为1，那么
$$
A^T = Q^TK
$$
中的元素的均值为0，方差为d.&lt;/p&gt;
&lt;p&gt;当d很大时，A中的元素的方差也很大，此时Softmax(A)的分布会趋于陡峭。&lt;/p&gt;
&lt;p&gt;也就是说Softmax(A)的分布和d有关，因此A中的内一个元素除以根号d_k后，方差又变为1.这使得Softmax(A)的分布陡峭程度与d解耦，从而使训练过程中梯度值保持稳定。&lt;/p&gt;
&lt;h3 id=&#34;自注意力机制与cnnrnn对比&#34;&gt;自注意力机制与CNN、RNN对比&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;自注意力机制与CNN对比&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在处理图像问题时，每一个像素点可以看作一个三维的向量，维度就是图像的通道数。所以图像也可以看作很多的向量输入到模型中。两者思想类似，都是希望网络能够不仅仅考虑某一个向量，而是考虑全局。对于CNN来说，矩形的感受野就是模型需要考虑的部分，但是self-attention是让模型自己决定有要考虑哪部分。&lt;/p&gt;
&lt;p&gt;CNN可以看作一种特殊的self-attention，self-attention可以看作一种复杂的CNN。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自注意力机制与RNN对比&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RNN和自注意力机制类似，都是接受一批输入向量，然后输出一批向量，但RNN只能接受前面的的输出作为输入，而self-attention可以同时关注到全部的向量。&lt;/p&gt;
&lt;h3 id=&#34;存在的问题&#34;&gt;存在的问题&lt;/h3&gt;
&lt;p&gt;虽然考虑了所有的输入向量，但没有考虑到向量的位置信息。在处理实际问题的过程中，同一个词在不同的位置可能有不同的含义。&lt;/p&gt;
&lt;p&gt;对此的改进方法是：&lt;strong&gt;位置编码&lt;/strong&gt; positional encoding&lt;/p&gt;
&lt;p&gt;对每一个输入向量加上一个位置向量e，通过e来表示位置信息，并带入self-attention层进行计算。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>SENet</title>
      <link>https://JuneCly.github.io/post/senet/</link>
      <pubDate>Fri, 28 Oct 2022 23:52:36 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/senet/</guid>
      
        <description>&lt;p&gt;Sequeeze-and-Excitation Networks, 简称SENet.获得了ImageNet最后一届竞赛图像分类任务的冠军。SENet通过Sequeeze和Excitation两个操作，让网络关注到channel之间的关系，希望模型可以自动学习到不同channel的重要特征。&lt;/p&gt;
&lt;h2 id=&#34;senet网络结构&#34;&gt;SENet网络结构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image1.png&#34; alt=&#34;SENet网络结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，是SENet Block结构。SENet的创新之处在于先对U进行一个全剧平均池化，再将得到的1x1xC数据经过两次全连接，最后用sigmoid函数将其控制在[0,1]，把得到的值作为scale乘到U的C个Channel上，作为下一级的输入数据。&lt;/p&gt;
&lt;p&gt;网络的SE操作目的是通过scale的值来将不同channel中的重要的特征进行增强，同时将不重要的特征进行减弱。&lt;/p&gt;
&lt;h3 id=&#34;sequeeze-操作&#34;&gt;Sequeeze 操作&lt;/h3&gt;
&lt;p&gt;对于Global Average Pooling，论文中作者使用了求平均的方法，将空间上所有点的信息都平均成了一个值。这是因为最终要得到的Scale值是要作用于整个通道，这就需要根据通道的整体信息来计算scale。具体计算公式如下：
$$
z_c = F_{sq}(u_c) = \frac{1}{W \times H} \sum_{i=1}^{W} \sum_{j=1}^{H} u_c(i, j)
$$&lt;/p&gt;
&lt;h3 id=&#34;excitation-操作&#34;&gt;Excitation 操作&lt;/h3&gt;
&lt;p&gt;作者通过两个全连接来实现Excitation，第一个全连接把C个通道压缩成了C/r个通道来降低计算量，第二个全连接层再恢复至C个通道。其中， r指代压缩的比例，文中选用的r=16。此时网络的整体性能和计算量最平衡。&lt;/p&gt;
&lt;h2 id=&#34;senet的应用&#34;&gt;SENet的应用&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image2.png&#34; alt=&#34;SENet实际应用&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是SENet在实际应用中与其他网络结合的例子。左图展示的是SE-Inception结构；右图展示的是SE-ResNet结构，需要注意的是，SE的scale部分放在了ResNet的add结构之前。下表是SE-ResNet的具体的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image3.png&#34; alt=&#34;SENet具体网络结构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;实验结果&#34;&gt;实验结果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image4.png&#34; alt=&#34;实验结果对比&#34;&gt;&lt;/p&gt;
&lt;p&gt;作者使用ImageNet数据集，分别对不同深度，不同类型的SENet结构和非SENet结构进行测试，上图是实验结果。从表中可以看出，在计算复杂度只有微小提高的情况下，SENet结构要比非SENet结构的效果更好。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>DenseNet</title>
      <link>https://JuneCly.github.io/post/densenet/</link>
      <pubDate>Fri, 28 Oct 2022 23:47:24 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/densenet/</guid>
      
        <description>&lt;p&gt;Dense Convolutional Network(DenseNet)是CVPR2017的最佳论文。与ResNet的思想类似，DenseNet建立了前面层与后面层之间的dense connection. 此外，DenseNet通过特征在通道上的链接来实现特征重用(feature reuse)。这些改进点让DenseNet在参数量和计算量更少的情况下实现了比ResNet更出色的效果。&lt;/p&gt;
&lt;p&gt;论文链接： &lt;a href=&#34;https://arxiv.org/pdf/1608.06993.pdf&#34;&gt; Densely Connected Convolutional Networks &lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;densenet网络结构&#34;&gt;DenseNet网络结构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image1.png&#34; alt=&#34;DenseNet网络结构图&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图为论文中给出的DenseNet网络结构图，共由五层的dense block组成，增长率k=4.&lt;/p&gt;
&lt;p&gt;DenseNet网络结构的特点在于每一层网络都会与前面的所有层在channel维度上连接到一起，连接方式为concat（而非Resnet的对应相加方式），concat后的特征图作为下一层的输入部分。因此对于一个层数为L的网络，网络中共包括L(L+1)/2个连接。&lt;/p&gt;
&lt;p&gt;通过cancat方式，DenseNet可以直接实现特征重用，让深层的网络也能够直接拿到之前几层网络的特征。这是DenseNet与ResNet的明显的区别之一。&lt;/p&gt;
&lt;h3 id=&#34;dense-connectivity&#34;&gt;Dense connectivity&lt;/h3&gt;
&lt;p&gt;为了进一步改善层与层之间的信息流，DenseNet提出了一种不同于ResNet的连接方式：引入了一种从任一层到后续所有层之间的直接连接。&lt;/p&gt;
&lt;p&gt;第l层接收了之前所有层的特征图作为输入：
$$
X_l = H_l([ X_0, X_1, &amp;hellip;, X_{l-1} ])
$$
其中[]方括号圈起的部分表示将0层至l-1层的特征图进行连接。正是由于这样的密集连接，作者将该网络命名为DenseNet.&lt;/p&gt;
&lt;h3 id=&#34;composite-function&#34;&gt;Composite function&lt;/h3&gt;
&lt;p&gt;DenseNet中将
$$
H_l(·)
$$
定义为三个连续操作的复合函数，这三个操作分别是：BN，ReLU以及3x3的卷积层。&lt;/p&gt;
&lt;h3 id=&#34;pooling-layers&#34;&gt;Pooling layers&lt;/h3&gt;
&lt;p&gt;为了便于进行下采样的操作，在DenseNet网络架构中作者将网络划分为多个密集连接的Dense Block.将这些dense blocks之间进行卷积和池化的层称为transition layers. 实验中使用的&lt;strong&gt;trasition layer&lt;/strong&gt;由一个BN层和一个1x1的卷积层以及一个2x2的平均池化层组成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image2.png&#34; alt=&#34;DenseNet网络结构图&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;growth-rate-k&#34;&gt;Growth rate k&lt;/h3&gt;
&lt;p&gt;k是一个超参数，叫做学习率。如果每个H函数生成k个特征图，那么第l层就有
$$
k_0 + k\times(l-1)
$$
输入特征图，其中k0是输入层的通道数量。&lt;/p&gt;
&lt;p&gt;一个相对小的学习率对于模型取得SOTA的水平起到了很好的作用。可以将特征图看作网络的全局的状态，而每一层都将自己的k的特征图添加到这一全局状态里。增长率u耳钉了每一层为全局状态提供多少新信息量。一旦将这些信息加入全局状态后，就可以从在该层之后的网络的任何位置对其进行访问。与传统的网络架构不同的是， 这种特征无需在层与层之间进行复制。&lt;/p&gt;
&lt;h3 id=&#34;bottleneck-layers&#34;&gt;Bottleneck layers&lt;/h3&gt;
&lt;p&gt;虽然学习率k通常取较小的数，即每层只提供k个特征图，但随着网络层数的增加，网络的参数量和计算量会变得很大。为此引入bottleneck layer，在每个3x3卷积之前引入1x1卷积作为“瓶颈”，进行将为操作，以减少输入特征图的数量。&lt;/p&gt;
&lt;p&gt;下表是DenseNet的具体网络结构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image3.png&#34; alt=&#34;DenseNet网络结构表&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-reuse&#34;&gt;Feature Reuse&lt;/h2&gt;
&lt;p&gt;DenseNet让每一层网络都能接触所有在其之前的网络层。通过实验画出热力图可以发现如下规律：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;较早层提取的特征直接被同一个Dense Block深层使用。&lt;/li&gt;
&lt;li&gt;第2个和第3个dense block内的层始终为transition layer输出最小的权重，这表明transition layer输出了很多冗余特征。&lt;/li&gt;
&lt;li&gt;尽管实验结果显示最终的分类层使用了整个dense block的权重，但似乎集中在最终的特征图上。这说明网络后期可能会产生一些更高级的特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experitments&#34;&gt;Experitments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image4.png&#34; alt=&#34;网络实验结果1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image5.png&#34; alt=&#34;网络实验结果2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;通过在具有同等大小的特征图的任意层之间增加连接的方式，DenseNet可以很轻易地拓展到数百层，并且没有显示出优化困难。DenseNet会随着参数数量的增加而不断提高准确性，并且不会出现性能下降或者过拟合的迹象。DenseNet使用了更少的的参数量和更少的计算量实现了SOTA水平。相信如果对DenseNet进一步调整超参数和学习率，该网络可以达到更好的效果。&lt;/p&gt;
&lt;h2 id=&#34;densenet相比于resnet的改进之处&#34;&gt;DenseNet相比于ResNet的改进之处：&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;比ResNet拥有更少的参数数量&lt;/li&gt;
&lt;li&gt;Bypass 加强了 feature reuse (特征重用)&lt;/li&gt;
&lt;li&gt;网络更易于训练，并具有一定的正则效果&lt;/li&gt;
&lt;li&gt;一定程度上缓解了梯度消失和网络退化问题&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title>ResNet</title>
      <link>https://JuneCly.github.io/post/resnet/</link>
      <pubDate>Sun, 11 Sep 2022 23:19:50 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/resnet/</guid>
      
        <description>&lt;p&gt;ResNet获得了2015年ILSVER比赛中图像分类第一名，目标检测第一名。获得COCO数据集比赛中目标检测第一名，图像分割第一名。该网络亮点在于引入残差模块来缓解随着网络深度的增加而出现的网络退化问题。&lt;/p&gt;
&lt;p&gt;论文：&lt;/p&gt;
&lt;p&gt;ResNet：&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ResNet V2：&lt;a href=&#34;https://arxiv.org/pdf/1603.05027.pdf&#34;&gt;Identity Mappings in Deep Residual Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;part---resnet&#34;&gt;Part Ⅰ : ResNet&lt;/h2&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;在ResNet出现之前，人们已经认同神经网络深度的增加可以提高模型的准确率这一观点。但在实际训练网络的过程中，常会出现这样的现象：模型的准确率随网络的深度增加上升到一定程度之后，准确率反而随网络的深度增加而下降。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet01.png&#34; alt=&#34;图1：网络退化现象&#34;&gt;&lt;/p&gt;
&lt;p&gt;造成这样现象的原因有两个：&lt;strong&gt;梯度爆炸/梯度消失问题&lt;/strong&gt;和&lt;strong&gt;网络退化问题&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;梯度爆炸梯度消失&#34;&gt;梯度爆炸/梯度消失&lt;/h4&gt;
&lt;p&gt;根据反向传播算法中的链式法则，如果层层之间的梯度均在（0，1）之间，层层缩小，那么就会出现&lt;strong&gt;梯度消失&lt;/strong&gt;。反之，如果层层传递的梯度大于1，那么经过层层扩大，就会出现&lt;strong&gt;梯度爆炸&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;要解决因网络深度增加而出现的梯度爆炸/梯度消失问题，可以使用&lt;strong&gt;BN&lt;/strong&gt;（Batch Normalization）来解决。&lt;/p&gt;
&lt;h4 id=&#34;网络退化&#34;&gt;网络退化&lt;/h4&gt;
&lt;p&gt;网络退化是指当神经网络越来越深的时候，反传回来的梯度之间的&lt;strong&gt;相关性会越来越差&lt;/strong&gt;，最后接近白噪声。为了缓解网络退化问题，作者引入了残差学习的思想。&lt;/p&gt;
&lt;h3 id=&#34;deep-residual-learning&#34;&gt;Deep Residual Learning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;”若某一较深的网络多出另一较浅网络的若干层有能力学习到恒等映射,那么这一较深网络训练得到的模型性能一定不会弱于该浅层网络。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者认为，如果能够通过某种方法，使得较深层的网络能够恒等映射浅层网络，那么其模型性能一定不会差于该浅层网络。这就是参差模块的核心思想：通过增加一条短路连接（shortcut connection）来实现网络的恒等映射（identity mapping）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet02.png&#34; alt=&#34;图2：残差模块&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图就是作者提出的参差模块：输入x经过两个卷积层后得到的结果F(x)，与经过shortcut connection的x相加后，再进行ReLU运算。&lt;/p&gt;
&lt;p&gt;对于一个堆积层结构（几层堆叠而成），当输入为x时其学习到的特征记为H(x)，现在我们希望其可以学习到残差 F(x)=H(x)−x ，这样原始的学习特征则是 F(x)+x 。对于神经网络来说，学习F(x)=0要比学习H(x)=x简单得多。因为对于前者而言，我们只需在学习F(x)=0时让其参数为0即可。&lt;/p&gt;
&lt;h3 id=&#34;network-architectures&#34;&gt;Network Architectures&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet03.png&#34; alt=&#34;图3：ResNet34网络架构及对比&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是ResNet34的网络结构，可以看出与同为34层的plain network相比，ResNet增加了许多shortcut connection。ResNet34根据颜色被分为5个模块（分别对应下表中展示的5个网络层）。&lt;/p&gt;
&lt;p&gt;需要注意的是，在第3、4、5模块（图中标注为绿色、红色、蓝色的部分）的起始位置的shortcut connection被标注为虚线。这是因为在这些地方，图像在经过residual模块时需要进行下采样操作。具体实现方法为：在residual分支中通过3x3conv, stride=2实现图像下采样；在shortcut分支中，此时的输入x则需要经过一个1x1conv的卷积来实现下采样。这样两个分支汇合时才能进行add操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet04.png&#34; alt=&#34;图4：ResNet网络架构表格&#34;&gt;&lt;/p&gt;
&lt;p&gt;上表是作者给出的不同层数的ResNet的具体网络结构。虽然不同网络的具体层数不同，但是总体的结构是一致的，即均是由5个卷积模块堆叠后接一个avg pool，然后fc 1000输出分类结果。&lt;/p&gt;
&lt;p&gt;在上表中，ResNet-50，ResNet-101和ResNet-152使用了一种名为瓶颈(bottleneck)的残差模块。我们下面将对这种结构进行进一步介绍。&lt;/p&gt;
&lt;h4 id=&#34;bottleneck&#34;&gt;bottleneck&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet05.png&#34; alt=&#34;图5：bottleneck结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中左边展示的是原本的参差模块结构，右边则是bottleneck结构。可以看出，bottleneck中，卷积层由两个3x3的卷积变为了1x1, 3x3, 1x1的三层卷积层。其中，前后两个1x1conv的作用是分别对图像进行降维，升维的操作。对于网络层数较多的ResNet-50/101/152来说，这种做法可以有效地减少参数量计算量。&lt;/p&gt;
&lt;h2 id=&#34;part---resnet-v2&#34;&gt;Part Ⅱ : ResNet V2&lt;/h2&gt;
&lt;p&gt;未完待续。。。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>GoogLeNet</title>
      <link>https://JuneCly.github.io/post/googlenet/</link>
      <pubDate>Thu, 01 Sep 2022 10:57:09 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/googlenet/</guid>
      
        <description>&lt;p&gt;GoogLeNet是google提出的基于Inception模块的串并联网络架构，并获得了2014年ILSVRC比赛的分类任务的冠军（同年该项亚军为VGG）。Inception模块及其迭代改进的版本可以提升模型的泛化能力、降低模型参数。&lt;/p&gt;
&lt;h2 id=&#34;part-1--goolenet&#34;&gt;Part 1 : GooLeNet&lt;/h2&gt;
&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34;&gt;Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;abstract--introduction&#34;&gt;Abstract &amp;amp; Introduction&lt;/h3&gt;
&lt;p&gt;GoogLeNet在ILSVRC14的分类和检测比赛中达到了新高度。这种网络结构提高了网络内计算资源的利用率，在保持计算量不变的同时增加网络的宽度和深度。GoogLeNet的网络架构思想基于Hebbian原则和多尺度处理的直觉，并且提出的Inception模块可以增加网络的深度。&lt;/p&gt;
&lt;h3 id=&#34;motivation-and-high-level-considerations&#34;&gt;Motivation and High Level Considerations&lt;/h3&gt;
&lt;p&gt;要提高深度神经网络的性能，最直接的方法就是增大规模。增大网络规模的方式有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增加网络深度（网络层数量）&lt;/li&gt;
&lt;li&gt;增加网络宽度（每层的神经元数量）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但直接增大网络规模的这种方法存在两个主要缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更大的规模意味着更多的参数，这会使得规模大的网络会更容易过拟合；&lt;/li&gt;
&lt;li&gt;更大的规模会造成计算资源的急剧增加，如果增加的部分的效率并不高，那么大量的计算资源都因此被浪费了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;解决这两个问题的最根本的方法就是将网络结构（包括卷积层内部）彻底从全连接（fully connected）变为稀疏连接（sparsely connected），对此有生物系统模拟和Arora等人的研究可佐证。但在实际应用中，将全连接变为稀疏连接后计算量并没有很大提升，这是因为现有硬件是针对密集矩阵进行计算优化的。由此提出Inception模块，在使用现有的计算稠密稀疏矩阵的硬件设备的条件下，利用稀疏连接提高网络的性能。&lt;/p&gt;
&lt;h3 id=&#34;architectural-details&#34;&gt;Architectural Details&lt;/h3&gt;
&lt;h4 id=&#34;inception-module&#34;&gt;Inception module&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet01.png&#34; alt=&#34;inception module a&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inception v1模块，将1x1，3x3，5x5 conv和3x3 pooling组成并联网络。一方面增加了网络的宽度，另一方面增加了网络对不同尺度的适应性（不同大小的卷积核支路所对应的感受野不同）。&lt;/p&gt;
&lt;h4 id=&#34;inception-module-with-dimension-reductions&#34;&gt;Inception module with dimension reductions&lt;/h4&gt;
&lt;p&gt;对于最初设计的Inception模块，虽然5x5的卷积核较少，但当网络达到一定规模后，仍然会产生巨大的计算量。为了解决这个问题，作者引入1x1的卷积核进行降维。对Inception v1的改进如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet03.png&#34; alt=&#34;inception module b&#34;&gt;&lt;/p&gt;
&lt;p&gt;改动后的结构有两个优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过降维，可以减少模型参数量；&lt;/li&gt;
&lt;li&gt;新增1x1卷积后可以带来更多的非线性变换，提高模型表达能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;
&lt;p&gt;GoogLeNet的网络细节如下表所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet02.png&#34; alt=&#34;GoogLeNet incarnation of the Inception architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;网络的一些特点如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络采用了模块化的结构，使用Inception mudule，便于修改网络结构。&lt;/li&gt;
&lt;li&gt;包括Inception在内的所有卷积都使用修正线性激活（ReLU）。&lt;/li&gt;
&lt;li&gt;网络使用average pooling代替了全连接层，但仍需要保留dropout。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;梯度回传&lt;/strong&gt;：为了避免梯度消失，网络中额外增加了2个辅助分类器（softmax）用于向前传播梯度。（辅助分类器只在训练时使用）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;将最佳稀疏结构稠密化是提高计算机视觉神经网络的有效方法。相比于浅且窄的网络，这种方法的优点在于只需适度增加计算量，性能就显著提升。&lt;/p&gt;
&lt;h2 id=&#34;part-2--inception系列&#34;&gt;Part 2 : Inception系列&lt;/h2&gt;
&lt;p&gt;论文：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.00567.pdf&#34;&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.07261.pdf&#34;&gt;Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;inception-v2&#34;&gt;Inception V2&lt;/h3&gt;
&lt;h4 id=&#34;4条设计原则&#34;&gt;4条设计原则&lt;/h4&gt;
&lt;p&gt;文章提出了4条设计原则，并依据这4条原则对Inception进行改进：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;网络浅层慎用bottleneck&lt;/li&gt;
&lt;li&gt;高维特征更适合在网络局部处理&lt;/li&gt;
&lt;li&gt;网络聚合可以通过低维嵌入&lt;/li&gt;
&lt;li&gt;平衡网络的深度和宽度&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;inception-v2-结构&#34;&gt;Inception V2 结构&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet04.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inception v2相对于v1做的改进是使用多个小卷积核代替一个大卷积核（如使用2个3x3的卷积代替原来的1个5x5卷积），这样可以有效减少模型的参数量，增加模型的深度。&lt;/p&gt;
&lt;p&gt;此外，Inception v2还引入了BN，加速网络训练，解决梯度消失。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet05.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;在此基础上，作者提出可以使用多个非对称的小尺寸卷积核堆叠，代替一个大卷积核（比如用1xn和nx1代替nxn）。需要注意的是，这种结构在前几层的效果并不好，当特征图的尺寸在12到20之间时使用的效果会更好一些。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet06.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;结合对称卷积和非对称卷积，增加网络宽度。&lt;/p&gt;
&lt;h3 id=&#34;inception-v3&#34;&gt;Inception V3&lt;/h3&gt;
&lt;p&gt;Inception v3中作者将7x7卷积分解成了3个3x3卷积，v3中使用的Aug loss里使用了BN进行regularization。&lt;/p&gt;
&lt;h4 id=&#34;降低特征图大小&#34;&gt;降低特征图大小&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet07.png&#34; alt=&#34;传统下采样&#34;&gt;&lt;/p&gt;
&lt;p&gt;传统的两种下采样方式如上图所示，要么先pooling再Inception（这种方法的缺点是池化会造成信息的丢失），要么先Inception再Pooling（这种方法的缺点是计算量增大）。故两种方法都不可取。作者提出了一种新的降低特征图大小的方法，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet08.png&#34; alt=&#34;inception下采样&#34;&gt;&lt;/p&gt;
&lt;p&gt;让池化和卷积并行执行（stride=2），最后再将特征图进行组合。&lt;/p&gt;
&lt;h3 id=&#34;inception-v4&#34;&gt;Inception V4&lt;/h3&gt;
&lt;h4 id=&#34;整体网络结构&#34;&gt;整体网络结构&lt;/h4&gt;
&lt;p&gt;相比于v2/v3，Inception v4的网络结构更加简介同意，并使用了更多的Inception模块。下图为Inception v4的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet09.png&#34; alt=&#34;inception v4整体网络结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;下图为Inception v4的stem模块。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet10.png&#34; alt=&#34;inception v4 stem module&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;inception-resnet&#34;&gt;Inception ResNet&lt;/h3&gt;
&lt;p&gt;Inception ResNet将Inception模块与残差连接思想结合，该系列有Inception-ResNet-v1和Inception-ResNet-v2，经验证，残差连接能够显著加速Inception的训练。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>VGG</title>
      <link>https://JuneCly.github.io/post/vgg/</link>
      <pubDate>Sat, 27 Aug 2022 13:36:02 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/vgg/</guid>
      
        <description>&lt;p&gt;VGG是由牛津大学视觉几何小组（&lt;strong&gt;V&lt;/strong&gt;isual &lt;strong&gt;G&lt;/strong&gt;eometry &lt;strong&gt;G&lt;/strong&gt;roup）提出的一种深层卷积网络，在2014年的ILSVRC比赛中获得了分类任务的亚军（同年该项冠军为GoogLeNet）和定位任务的冠军。&lt;/p&gt;
&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34;&gt;Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-vgg网络架构&#34;&gt;1. VGG网络架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG02.png&#34; alt=&#34;VGG convnet configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;VGG网络由5层卷积层，3层全连接层以及softmax输出层构成，层与层之间使用max pooling（最大池化）分开。作者根据具体卷积层的不同，共设计了6种网络结构，如上图所示，分别是A、A-LRN、B、C、D、E。这6种结构的网络深度从11层到19层。其中，D和E结构即是我们所熟知的VGG16和VGG19。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG01.jpg&#34; alt=&#34;VGG16&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图为VGG16的网络结构图。第1层卷积层由2个conv3-64组成，第2层卷积层由2个conv3-128组成，第3层卷积层由3个conv3-256组成，第4层卷积层由3个conv3-512组成，第5层卷积层由3个conv3-512组成，后接3个全连接层，前两个FC输出通道数为4096，后1个FC输出通道数为1000，最后经过softmax。共计16层。&lt;/p&gt;
&lt;h2 id=&#34;2-网络特点&#34;&gt;2. 网络特点&lt;/h2&gt;
&lt;h3 id=&#34;21-结构简单&#34;&gt;2.1 结构简单&lt;/h3&gt;
&lt;p&gt;作者在文中提出的6种网络结构虽在细节上有所不同，但VGG总体的网络结构保持一致，即5卷积+3全连接+softmax，并且层与层之间使用maxpool分开。所有隐层的激活函数均为ReLU。&lt;/p&gt;
&lt;h3 id=&#34;22-小卷积核&#34;&gt;2.2 小卷积核&lt;/h3&gt;
&lt;p&gt;VGG使用多个小卷积核（3x3）来代替一个较大的卷积层。例如，2个3x3的卷积层相当于1个5x5卷积，3个3x3卷积相当于1个7x7卷积。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG03.png&#34; alt=&#34;感受野&#34;&gt;&lt;/p&gt;
&lt;p&gt;使用小卷积核有两点好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少模型参数量&lt;/li&gt;
&lt;li&gt;因卷积核变小二导致的层数增加，可以增加模型的非线性表达能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23-小池化核&#34;&gt;2.3 小池化核&lt;/h3&gt;
&lt;p&gt;相比于AlexNet的3x3的池化核，VGG全部采用2x2的池化核。&lt;/p&gt;
&lt;h3 id=&#34;24-通道数更多&#34;&gt;2.4 通道数更多&lt;/h3&gt;
&lt;p&gt;更多的通道数可以表示更多的图像特征，VGG网络每个卷积层都对通道数进行了翻倍操作，直至增大到512个通道数。这样就可以提取出更多的信息。&lt;/p&gt;
&lt;h3 id=&#34;25-全连接转卷积&#34;&gt;2.5 全连接转卷积&lt;/h3&gt;
&lt;p&gt;VGG在&lt;strong&gt;测试阶段&lt;/strong&gt;将训练阶段的3个全连接层替换为3个卷积层，这样做的优点是可以处理任意大小尺寸的图片输入，并减少特征位置对分类的影响。（注：这个特点是作者参考了OverFeat的工作思路。）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG04.png&#34; alt=&#34;测试阶段全连接转卷积&#34;&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AlexNet</title>
      <link>https://JuneCly.github.io/post/alexnet/</link>
      <pubDate>Mon, 22 Aug 2022 15:57:49 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/alexnet/</guid>
      
        <description>&lt;p&gt;AlexNet在2012的ImageNet的竞赛中获得了冠军，该模型由Hintom和他的学生Alex Krizhevsky等人提出。 AlexNet由5个卷积层和3个全连接层构成，并首次使用ReLU、LRN、Dropout等技巧来提高网络的准确率。&lt;/p&gt;
&lt;p&gt;论文：&lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&#34;&gt;AlexNet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;alexnet架构&#34;&gt;AlexNet架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/AlexNet01.png&#34; alt=&#34;AlexNet架构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tricks&#34;&gt;Tricks&lt;/h2&gt;
&lt;p&gt;AlexNet应用的一些特殊的方法（按重要性进行排序）&lt;/p&gt;
&lt;h3 id=&#34;relu&#34;&gt;ReLU&lt;/h3&gt;
&lt;p&gt;AlexNet首次使用ReLU函数代替sigmoid和Tanh作为激活函数，这是因为相比于后两个函数，ReLU可以更好的解决梯度消失的问题。当ReLU函数的输入大于0时直接返回原值，输入小于0时则返回0。这样既可以利用正输入的梯度为1这个特点解决梯度消失问题，也可以利用负输入的输出为0这一特性，增加模型的稀疏性表示，进而加速并简化模型。（需要注意的是，ReLU常作为CNN的激活函数，但通常情况下并不适合作为RNN的激活函数）&lt;/p&gt;
&lt;h3 id=&#34;多gpu训练&#34;&gt;多GPU训练&lt;/h3&gt;
&lt;p&gt;由于GTX 580 GPU只有3GB的内存，因此将AlexNet网络分布在两块GPU上进行训练。需要额外注意的是，在训练时，GPU只在某些特定的层上进行通信，比如第3层的核会将第2层的所有核映射作为输入，第4层的核只将位于同一GPU上的第3层的核映射作为输入（这一点在网络架构图中可以看出）。&lt;/p&gt;
&lt;h3 id=&#34;lrn-局部响应归一化&#34;&gt;LRN (局部响应归一化)&lt;/h3&gt;
&lt;p&gt;局部响应归一化 (Local Response Normalization) 通过模拟生物医学中的“侧抑制”（被激活的神经元会抑制周围的神经元）来实现局部抑制，增强模型的泛化能力。（注：现在LRN已经逐渐被BN所取代）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;归一化的优点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加快收敛速度&lt;/li&gt;
&lt;li&gt;提高模型精度&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;重叠池化&#34;&gt;重叠池化&lt;/h3&gt;
&lt;p&gt;通常池化的窗口大小z与步长s相等，因此池化作用的区域不存在重叠的部分。但重叠池化令z&amp;gt;s，使得池化区域之间存在重叠的部分。AlexNet这篇论文指出，采用重叠池化的模型更不容易过拟合。&lt;/p&gt;
&lt;h2 id=&#34;减少过拟合&#34;&gt;减少过拟合&lt;/h2&gt;
&lt;p&gt;AlexNet主要使用两种方法来克服过拟合。&lt;/p&gt;
&lt;h3 id=&#34;数据增强&#34;&gt;数据增强&lt;/h3&gt;
&lt;p&gt;AlexNet用两种方式实现数据增强。因这两种方式都是由原始图像经非常少的计算量产生变换得到的图像，因此无需存储在硬盘上。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;随机裁剪、水平翻转&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改变训练图像的RGB通道的强度&lt;/strong&gt;：对RGB像素值集合执行PCA，并对主成分做一个标准差为0.1的高斯扰动。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;
&lt;p&gt;在训练过程中以p=0.5的概率对每个隐层神经元的输出设为0，以此丢弃部分神经元。这些被丢弃的神经元将不再进行前向传播并且不参与反向传播。采用droput方法可以避免过拟合。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Test</title>
      <link>https://JuneCly.github.io/post/test/</link>
      <pubDate>Mon, 15 Aug 2022 15:33:51 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/test/</guid>
      
        <description>&lt;p&gt;测试&lt;/p&gt;
&lt;h2 id=&#34;paragraph-and-line-breaks&#34;&gt;Paragraph and line breaks&lt;/h2&gt;
&lt;p&gt;A paragraph is simply one or more consecutive lines of text. In markdown source code, paragraphs are separated by more than one blank lines. In Typora, you only need to press &lt;code&gt;Return&lt;/code&gt; to create a new paragraph.&lt;/p&gt;
&lt;p&gt;Press &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Return&lt;/code&gt; to create a single line break. However, most markdown parser will ignore single line break, to make other markdown parsers recognize your line break, you can leave two whitespace at the end of the line, or insert &lt;code&gt;&amp;lt;br/&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;headers&#34;&gt;Headers&lt;/h2&gt;
&lt;p&gt;Headers use 1-6 hash characters at the start of the line, corresponding to header levels 1-6. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# This is an H1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## This is an H2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;###### This is an H6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In typora, input ‘#’s followed by title content, and press &lt;code&gt;Return&lt;/code&gt; key will create a header.&lt;/p&gt;
&lt;h2 id=&#34;blockquotes&#34;&gt;Blockquotes&lt;/h2&gt;
&lt;p&gt;Markdown uses email-style &amp;gt; characters for block quoting. They are presented as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a blockquote with two paragraphs. This is first paragraph.&lt;/p&gt;
&lt;p&gt;This is second pragraph.Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&lt;/p&gt;
&lt;p&gt;This is another blockquote with one paragraph. There is three empty line to seperate two blockquote.&lt;/p&gt;
&lt;p&gt;这是一段中文测试。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In typora, just input ‘&amp;gt;’ followed by quote contents a block quote is  generated. Typora will insert proper ‘&amp;gt;’ or line break for you. Block quote inside anther block quote is allowed by adding additional levels of ‘&amp;gt;’.&lt;/p&gt;
&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;Input &lt;code&gt;* list item 1&lt;/code&gt; will create an un-ordered list, the &lt;code&gt;*&lt;/code&gt; symbol can be replace with &lt;code&gt;+&lt;/code&gt; or &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Input &lt;code&gt;1. list item 1&lt;/code&gt; will create an ordered list, their markdown source code is like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;task-list&#34;&gt;Task List&lt;/h2&gt;
&lt;p&gt;Task lists are lists with items marked as either &lt;code&gt;[ ]&lt;/code&gt; or &lt;code&gt;[x]&lt;/code&gt; (incomplete or complete). For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; a task list item&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; list syntax required&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; normal &lt;strong&gt;formatting&lt;/strong&gt;, @mentions, #1234 refs&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; incomplete&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; completed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can change the complete/incomplete state by click the checkbox before the item.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
