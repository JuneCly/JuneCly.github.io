<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chen LY</title>
    <link>https://JuneCly.github.io/</link>
    <description>Recent content on Chen LY</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 28 Oct 2022 23:56:33 +0800</lastBuildDate>
    
        <atom:link href="https://JuneCly.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Self Attention</title>
      <link>https://JuneCly.github.io/post/self-attention/</link>
      <pubDate>Fri, 28 Oct 2022 23:56:33 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/self-attention/</guid>
      
        <description>&lt;p&gt;对于Transformer中Self-attention机制的理解&lt;/p&gt;
&lt;h2 id=&#34;attention-mechanism&#34;&gt;Attention Mechanism&lt;/h2&gt;
&lt;h3 id=&#34;attention机制理解&#34;&gt;attention机制理解&lt;/h3&gt;
&lt;p&gt;将人的感知方式，注意力的行为应用在机器上，让机器学会感知数据中重要的和不重要的部分。&lt;/p&gt;
&lt;h3 id=&#34;attention分类&#34;&gt;attention分类&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Soft/Global Attention（软注意/全局注意）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对每个输入项分配的权重在0-1之间，也就是某些部分关注的多一点，某些部分关注的少一点，因为对大部分信息都关注，但关注程度不同。所以计算量相对来说较大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hard/Local Attention （硬注意/局部注意）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对每个输入项分配的权重非0即1，只关注需要的部分，舍弃掉无关的部分。好处是可以减少一定的时间和计算成本，缺点是有可能丢失信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self/Intra Attention （自注意力机制/内注意）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对每个输入项分配的权重取决于输入项之间的相互作用，即通过输入项内部的“表决”来决定应该关注哪些输入项。与前两者相比，自注意力机制在处理很长的输入时，具有并行计算的优势。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-attention&#34;&gt;Self-Attention&lt;/h2&gt;
&lt;h3 id=&#34;self-attention的挑战与目标&#34;&gt;Self-Attention的挑战与目标&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The animal didn’t cross the street because it was too tired&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The animal didn’t cross the street because it was too wide&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;两个句子中的it指代的对象并不一样，但是这对于机器来说很难判断。&lt;/p&gt;
&lt;p&gt;self-attention机制通过计算单词it与其他词之间的联系得知it的具体指代。&lt;/p&gt;
&lt;h3 id=&#34;self-attention的计算过程概述&#34;&gt;Self-Attention的计算过程概述&lt;/h3&gt;
&lt;p&gt;对于一系列输入向量a，经过self-attention 后得到一系列向量b，输出的每个token是考虑了输入的所有token后得到的。所以4个向量token对应4个向量的token&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于每个向量a，分别乘上三个矩阵系数W得到三个值：Q, K, V，分别表示query, key, value&lt;/li&gt;
&lt;li&gt;利用得到的Q和K计算每两个输入向量之间的相关性，也就是计算attention的a，常用点乘方法计算。A= K·Q，矩阵A中的每一个值记录了对应的两个输入向量的Attention大小&lt;/li&gt;
&lt;li&gt;对A矩阵进行softmax或者Relu得到A&#39;&lt;/li&gt;
&lt;li&gt;利用得到的A&amp;rsquo;和V计算得到每个输入向量a对应的self-attention层的输出向量b&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;attention核心公式的理解&#34;&gt;Attention核心公式的理解&lt;/h3&gt;
&lt;p&gt;$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$&lt;/p&gt;
&lt;h4 id=&#34;1对公式计算步骤的理解&#34;&gt;1、对公式计算步骤的理解&lt;/h4&gt;
&lt;p&gt;抛开Q，K，V三个矩阵，self-attention的最原始形态的公式如下：
$$
Softmax(XX^T)X
$$
其中
$$
XX^T代表什么
$$
一个矩阵是由一些向量组成的，当一个矩阵乘以自己的转置矩阵时，（将矩阵中的向量以多个行向量组成的方式表示）可以看作这些向量分别与其他向量计算内积。&lt;/p&gt;
&lt;p&gt;而向量内积的几何意义是：表征两个向量的家教，即表征一个向量在另一个向量上的投影。投影的值越大，说明两个向量的相关度越高。&lt;/p&gt;
&lt;p&gt;如果两个向量的夹角是90°，那么这两个向量线性无关，完全没有相关性。&lt;/p&gt;
&lt;p&gt;将词向量代入其中进行理解，若两个词向量计算内积得到的投影的值越大，可以理解为两个词向量的相关性就越高。&lt;/p&gt;
&lt;p&gt;接下来，进一步理解
$$
Softmax(XX^T)中softmax的意义
$$
softmax的意义是进行归一化。而Attention 机制的核心是正是&lt;strong&gt;加权求和&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;接下来我们再看这个公式
$$
Softmax(XX^T)X
$$
我们取经过内积与softmax运算后的一个行向量举例，将其与X的一个列向量相乘，新得到的向量与原来X的向量维度相同。&lt;/p&gt;
&lt;p&gt;在新的向量中，每一个维度的数值都是由三个词向量在这一维度的数值加权求和得到的，这个新的行向量就是原来的“早”字向量经过注意力机制加权求和之后的表示。&lt;/p&gt;
&lt;h4 id=&#34;2对qkv矩阵的理解&#34;&gt;2、对Q、K、V矩阵的理解&lt;/h4&gt;
&lt;p&gt;Q、K、V矩阵都是矩阵X与权重矩阵W的乘积得到的，即他们&lt;strong&gt;本质上都是X的线性变换&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为什么不直接用矩阵X进行计算呢？是为了提升模型的拟合能力。因为矩阵W是可以训练的，可以起到一个缓冲的作用。&lt;/p&gt;
&lt;h4 id=&#34;3对根号d_k的理解&#34;&gt;3、对根号d_k的理解&lt;/h4&gt;
&lt;p&gt;$$
\sqrt{d_k}
$$&lt;/p&gt;
&lt;p&gt;假设Q，K里的元素均值为0，方差为1，那么
$$
A^T = Q^TK
$$
中的元素的均值为0，方差为d.&lt;/p&gt;
&lt;p&gt;当d很大时，A中的元素的方差也很大，此时Softmax(A)的分布会趋于陡峭。&lt;/p&gt;
&lt;p&gt;也就是说Softmax(A)的分布和d有关，因此A中的内一个元素除以根号d_k后，方差又变为1.这使得Softmax(A)的分布陡峭程度与d解耦，从而使训练过程中梯度值保持稳定。&lt;/p&gt;
&lt;h3 id=&#34;自注意力机制与cnnrnn对比&#34;&gt;自注意力机制与CNN、RNN对比&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;自注意力机制与CNN对比&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在处理图像问题时，每一个像素点可以看作一个三维的向量，维度就是图像的通道数。所以图像也可以看作很多的向量输入到模型中。两者思想类似，都是希望网络能够不仅仅考虑某一个向量，而是考虑全局。对于CNN来说，矩形的感受野就是模型需要考虑的部分，但是self-attention是让模型自己决定有要考虑哪部分。&lt;/p&gt;
&lt;p&gt;CNN可以看作一种特殊的self-attention，self-attention可以看作一种复杂的CNN。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自注意力机制与RNN对比&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RNN和自注意力机制类似，都是接受一批输入向量，然后输出一批向量，但RNN只能接受前面的的输出作为输入，而self-attention可以同时关注到全部的向量。&lt;/p&gt;
&lt;h3 id=&#34;存在的问题&#34;&gt;存在的问题&lt;/h3&gt;
&lt;p&gt;虽然考虑了所有的输入向量，但没有考虑到向量的位置信息。在处理实际问题的过程中，同一个词在不同的位置可能有不同的含义。&lt;/p&gt;
&lt;p&gt;对此的改进方法是：&lt;strong&gt;位置编码&lt;/strong&gt; positional encoding&lt;/p&gt;
&lt;p&gt;对每一个输入向量加上一个位置向量e，通过e来表示位置信息，并带入self-attention层进行计算。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>SENet</title>
      <link>https://JuneCly.github.io/post/senet/</link>
      <pubDate>Fri, 28 Oct 2022 23:52:36 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/senet/</guid>
      
        <description>&lt;p&gt;Sequeeze-and-Excitation Networks, 简称SENet.获得了ImageNet最后一届竞赛图像分类任务的冠军。SENet通过Sequeeze和Excitation两个操作，让网络关注到channel之间的关系，希望模型可以自动学习到不同channel的重要特征。&lt;/p&gt;
&lt;h2 id=&#34;senet网络结构&#34;&gt;SENet网络结构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image1.png&#34; alt=&#34;SENet网络结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，是SENet Block结构。SENet的创新之处在于先对U进行一个全剧平均池化，再将得到的1x1xC数据经过两次全连接，最后用sigmoid函数将其控制在[0,1]，把得到的值作为scale乘到U的C个Channel上，作为下一级的输入数据。&lt;/p&gt;
&lt;p&gt;网络的SE操作目的是通过scale的值来将不同channel中的重要的特征进行增强，同时将不重要的特征进行减弱。&lt;/p&gt;
&lt;h3 id=&#34;sequeeze-操作&#34;&gt;Sequeeze 操作&lt;/h3&gt;
&lt;p&gt;对于Global Average Pooling，论文中作者使用了求平均的方法，将空间上所有点的信息都平均成了一个值。这是因为最终要得到的Scale值是要作用于整个通道，这就需要根据通道的整体信息来计算scale。具体计算公式如下：
$$
z_c = F_{sq}(u_c) = \frac{1}{W \times H} \sum_{i=1}^{W} \sum_{j=1}^{H} u_c(i, j)
$$&lt;/p&gt;
&lt;h3 id=&#34;excitation-操作&#34;&gt;Excitation 操作&lt;/h3&gt;
&lt;p&gt;作者通过两个全连接来实现Excitation，第一个全连接把C个通道压缩成了C/r个通道来降低计算量，第二个全连接层再恢复至C个通道。其中， r指代压缩的比例，文中选用的r=16。此时网络的整体性能和计算量最平衡。&lt;/p&gt;
&lt;h2 id=&#34;senet的应用&#34;&gt;SENet的应用&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image2.png&#34; alt=&#34;SENet实际应用&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是SENet在实际应用中与其他网络结合的例子。左图展示的是SE-Inception结构；右图展示的是SE-ResNet结构，需要注意的是，SE的scale部分放在了ResNet的add结构之前。下表是SE-ResNet的具体的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image3.png&#34; alt=&#34;SENet具体网络结构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;实验结果&#34;&gt;实验结果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/SENetPic/SENet-Image4.png&#34; alt=&#34;实验结果对比&#34;&gt;&lt;/p&gt;
&lt;p&gt;作者使用ImageNet数据集，分别对不同深度，不同类型的SENet结构和非SENet结构进行测试，上图是实验结果。从表中可以看出，在计算复杂度只有微小提高的情况下，SENet结构要比非SENet结构的效果更好。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>DenseNet</title>
      <link>https://JuneCly.github.io/post/densenet/</link>
      <pubDate>Fri, 28 Oct 2022 23:47:24 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/densenet/</guid>
      
        <description>&lt;p&gt;Dense Convolutional Network(DenseNet)是CVPR2017的最佳论文。与ResNet的思想类似，DenseNet建立了前面层与后面层之间的dense connection. 此外，DenseNet通过特征在通道上的链接来实现特征重用(feature reuse)。这些改进点让DenseNet在参数量和计算量更少的情况下实现了比ResNet更出色的效果。&lt;/p&gt;
&lt;p&gt;论文链接： &lt;a href=&#34;https://arxiv.org/pdf/1608.06993.pdf&#34;&gt; Densely Connected Convolutional Networks &lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;densenet网络结构&#34;&gt;DenseNet网络结构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image1.png&#34; alt=&#34;DenseNet网络结构图&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图为论文中给出的DenseNet网络结构图，共由五层的dense block组成，增长率k=4.&lt;/p&gt;
&lt;p&gt;DenseNet网络结构的特点在于每一层网络都会与前面的所有层在channel维度上连接到一起，连接方式为concat（而非Resnet的对应相加方式），concat后的特征图作为下一层的输入部分。因此对于一个层数为L的网络，网络中共包括L(L+1)/2个连接。&lt;/p&gt;
&lt;p&gt;通过cancat方式，DenseNet可以直接实现特征重用，让深层的网络也能够直接拿到之前几层网络的特征。这是DenseNet与ResNet的明显的区别之一。&lt;/p&gt;
&lt;h3 id=&#34;dense-connectivity&#34;&gt;Dense connectivity&lt;/h3&gt;
&lt;p&gt;为了进一步改善层与层之间的信息流，DenseNet提出了一种不同于ResNet的连接方式：引入了一种从任一层到后续所有层之间的直接连接。&lt;/p&gt;
&lt;p&gt;第l层接收了之前所有层的特征图作为输入：
$$
X_l = H_l([ X_0, X_1, &amp;hellip;, X_{l-1} ])
$$
其中[]方括号圈起的部分表示将0层至l-1层的特征图进行连接。正是由于这样的密集连接，作者将该网络命名为DenseNet.&lt;/p&gt;
&lt;h3 id=&#34;composite-function&#34;&gt;Composite function&lt;/h3&gt;
&lt;p&gt;DenseNet中将
$$
H_l(·)
$$
定义为三个连续操作的复合函数，这三个操作分别是：BN，ReLU以及3x3的卷积层。&lt;/p&gt;
&lt;h3 id=&#34;pooling-layers&#34;&gt;Pooling layers&lt;/h3&gt;
&lt;p&gt;为了便于进行下采样的操作，在DenseNet网络架构中作者将网络划分为多个密集连接的Dense Block.将这些dense blocks之间进行卷积和池化的层称为transition layers. 实验中使用的&lt;strong&gt;trasition layer&lt;/strong&gt;由一个BN层和一个1x1的卷积层以及一个2x2的平均池化层组成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image2.png&#34; alt=&#34;DenseNet网络结构图&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;growth-rate-k&#34;&gt;Growth rate k&lt;/h3&gt;
&lt;p&gt;k是一个超参数，叫做学习率。如果每个H函数生成k个特征图，那么第l层就有
$$
k_0 + k\times(l-1)
$$
输入特征图，其中k0是输入层的通道数量。&lt;/p&gt;
&lt;p&gt;一个相对小的学习率对于模型取得SOTA的水平起到了很好的作用。可以将特征图看作网络的全局的状态，而每一层都将自己的k的特征图添加到这一全局状态里。增长率u耳钉了每一层为全局状态提供多少新信息量。一旦将这些信息加入全局状态后，就可以从在该层之后的网络的任何位置对其进行访问。与传统的网络架构不同的是， 这种特征无需在层与层之间进行复制。&lt;/p&gt;
&lt;h3 id=&#34;bottleneck-layers&#34;&gt;Bottleneck layers&lt;/h3&gt;
&lt;p&gt;虽然学习率k通常取较小的数，即每层只提供k个特征图，但随着网络层数的增加，网络的参数量和计算量会变得很大。为此引入bottleneck layer，在每个3x3卷积之前引入1x1卷积作为“瓶颈”，进行将为操作，以减少输入特征图的数量。&lt;/p&gt;
&lt;p&gt;下表是DenseNet的具体网络结构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image3.png&#34; alt=&#34;DenseNet网络结构表&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-reuse&#34;&gt;Feature Reuse&lt;/h2&gt;
&lt;p&gt;DenseNet让每一层网络都能接触所有在其之前的网络层。通过实验画出热力图可以发现如下规律：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;较早层提取的特征直接被同一个Dense Block深层使用。&lt;/li&gt;
&lt;li&gt;第2个和第3个dense block内的层始终为transition layer输出最小的权重，这表明transition layer输出了很多冗余特征。&lt;/li&gt;
&lt;li&gt;尽管实验结果显示最终的分类层使用了整个dense block的权重，但似乎集中在最终的特征图上。这说明网络后期可能会产生一些更高级的特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experitments&#34;&gt;Experitments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image4.png&#34; alt=&#34;网络实验结果1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/DenseNetPic/DenseNet-Image5.png&#34; alt=&#34;网络实验结果2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;通过在具有同等大小的特征图的任意层之间增加连接的方式，DenseNet可以很轻易地拓展到数百层，并且没有显示出优化困难。DenseNet会随着参数数量的增加而不断提高准确性，并且不会出现性能下降或者过拟合的迹象。DenseNet使用了更少的的参数量和更少的计算量实现了SOTA水平。相信如果对DenseNet进一步调整超参数和学习率，该网络可以达到更好的效果。&lt;/p&gt;
&lt;h2 id=&#34;densenet相比于resnet的改进之处&#34;&gt;DenseNet相比于ResNet的改进之处：&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;比ResNet拥有更少的参数数量&lt;/li&gt;
&lt;li&gt;Bypass 加强了 feature reuse (特征重用)&lt;/li&gt;
&lt;li&gt;网络更易于训练，并具有一定的正则效果&lt;/li&gt;
&lt;li&gt;一定程度上缓解了梯度消失和网络退化问题&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title>ResNet</title>
      <link>https://JuneCly.github.io/post/resnet/</link>
      <pubDate>Sun, 11 Sep 2022 23:19:50 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/resnet/</guid>
      
        <description>&lt;p&gt;ResNet获得了2015年ILSVER比赛中图像分类第一名，目标检测第一名。获得COCO数据集比赛中目标检测第一名，图像分割第一名。该网络亮点在于引入残差模块来缓解随着网络深度的增加而出现的网络退化问题。&lt;/p&gt;
&lt;p&gt;论文：&lt;/p&gt;
&lt;p&gt;ResNet：&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ResNet V2：&lt;a href=&#34;https://arxiv.org/pdf/1603.05027.pdf&#34;&gt;Identity Mappings in Deep Residual Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;part---resnet&#34;&gt;Part Ⅰ : ResNet&lt;/h2&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;在ResNet出现之前，人们已经认同神经网络深度的增加可以提高模型的准确率这一观点。但在实际训练网络的过程中，常会出现这样的现象：模型的准确率随网络的深度增加上升到一定程度之后，准确率反而随网络的深度增加而下降。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet01.png&#34; alt=&#34;图1：网络退化现象&#34;&gt;&lt;/p&gt;
&lt;p&gt;造成这样现象的原因有两个：&lt;strong&gt;梯度爆炸/梯度消失问题&lt;/strong&gt;和&lt;strong&gt;网络退化问题&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;梯度爆炸梯度消失&#34;&gt;梯度爆炸/梯度消失&lt;/h4&gt;
&lt;p&gt;根据反向传播算法中的链式法则，如果层层之间的梯度均在（0，1）之间，层层缩小，那么就会出现&lt;strong&gt;梯度消失&lt;/strong&gt;。反之，如果层层传递的梯度大于1，那么经过层层扩大，就会出现&lt;strong&gt;梯度爆炸&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;要解决因网络深度增加而出现的梯度爆炸/梯度消失问题，可以使用&lt;strong&gt;BN&lt;/strong&gt;（Batch Normalization）来解决。&lt;/p&gt;
&lt;h4 id=&#34;网络退化&#34;&gt;网络退化&lt;/h4&gt;
&lt;p&gt;网络退化是指当神经网络越来越深的时候，反传回来的梯度之间的&lt;strong&gt;相关性会越来越差&lt;/strong&gt;，最后接近白噪声。为了缓解网络退化问题，作者引入了残差学习的思想。&lt;/p&gt;
&lt;h3 id=&#34;deep-residual-learning&#34;&gt;Deep Residual Learning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;”若某一较深的网络多出另一较浅网络的若干层有能力学习到恒等映射,那么这一较深网络训练得到的模型性能一定不会弱于该浅层网络。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者认为，如果能够通过某种方法，使得较深层的网络能够恒等映射浅层网络，那么其模型性能一定不会差于该浅层网络。这就是参差模块的核心思想：通过增加一条短路连接（shortcut connection）来实现网络的恒等映射（identity mapping）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet02.png&#34; alt=&#34;图2：残差模块&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图就是作者提出的参差模块：输入x经过两个卷积层后得到的结果F(x)，与经过shortcut connection的x相加后，再进行ReLU运算。&lt;/p&gt;
&lt;p&gt;对于一个堆积层结构（几层堆叠而成），当输入为x时其学习到的特征记为H(x)，现在我们希望其可以学习到残差 F(x)=H(x)−x ，这样原始的学习特征则是 F(x)+x 。对于神经网络来说，学习F(x)=0要比学习H(x)=x简单得多。因为对于前者而言，我们只需在学习F(x)=0时让其参数为0即可。&lt;/p&gt;
&lt;h3 id=&#34;network-architectures&#34;&gt;Network Architectures&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet03.png&#34; alt=&#34;图3：ResNet34网络架构及对比&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是ResNet34的网络结构，可以看出与同为34层的plain network相比，ResNet增加了许多shortcut connection。ResNet34根据颜色被分为5个模块（分别对应下表中展示的5个网络层）。&lt;/p&gt;
&lt;p&gt;需要注意的是，在第3、4、5模块（图中标注为绿色、红色、蓝色的部分）的起始位置的shortcut connection被标注为虚线。这是因为在这些地方，图像在经过residual模块时需要进行下采样操作。具体实现方法为：在residual分支中通过3x3conv, stride=2实现图像下采样；在shortcut分支中，此时的输入x则需要经过一个1x1conv的卷积来实现下采样。这样两个分支汇合时才能进行add操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet04.png&#34; alt=&#34;图4：ResNet网络架构表格&#34;&gt;&lt;/p&gt;
&lt;p&gt;上表是作者给出的不同层数的ResNet的具体网络结构。虽然不同网络的具体层数不同，但是总体的结构是一致的，即均是由5个卷积模块堆叠后接一个avg pool，然后fc 1000输出分类结果。&lt;/p&gt;
&lt;p&gt;在上表中，ResNet-50，ResNet-101和ResNet-152使用了一种名为瓶颈(bottleneck)的残差模块。我们下面将对这种结构进行进一步介绍。&lt;/p&gt;
&lt;h4 id=&#34;bottleneck&#34;&gt;bottleneck&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/ResNet/ResNet05.png&#34; alt=&#34;图5：bottleneck结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中左边展示的是原本的参差模块结构，右边则是bottleneck结构。可以看出，bottleneck中，卷积层由两个3x3的卷积变为了1x1, 3x3, 1x1的三层卷积层。其中，前后两个1x1conv的作用是分别对图像进行降维，升维的操作。对于网络层数较多的ResNet-50/101/152来说，这种做法可以有效地减少参数量计算量。&lt;/p&gt;
&lt;h2 id=&#34;part---resnet-v2&#34;&gt;Part Ⅱ : ResNet V2&lt;/h2&gt;
&lt;p&gt;未完待续。。。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>GoogLeNet</title>
      <link>https://JuneCly.github.io/post/googlenet/</link>
      <pubDate>Thu, 01 Sep 2022 10:57:09 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/googlenet/</guid>
      
        <description>&lt;p&gt;GoogLeNet是google提出的基于Inception模块的串并联网络架构，并获得了2014年ILSVRC比赛的分类任务的冠军（同年该项亚军为VGG）。Inception模块及其迭代改进的版本可以提升模型的泛化能力、降低模型参数。&lt;/p&gt;
&lt;h2 id=&#34;part-1--goolenet&#34;&gt;Part 1 : GooLeNet&lt;/h2&gt;
&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34;&gt;Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;abstract--introduction&#34;&gt;Abstract &amp;amp; Introduction&lt;/h3&gt;
&lt;p&gt;GoogLeNet在ILSVRC14的分类和检测比赛中达到了新高度。这种网络结构提高了网络内计算资源的利用率，在保持计算量不变的同时增加网络的宽度和深度。GoogLeNet的网络架构思想基于Hebbian原则和多尺度处理的直觉，并且提出的Inception模块可以增加网络的深度。&lt;/p&gt;
&lt;h3 id=&#34;motivation-and-high-level-considerations&#34;&gt;Motivation and High Level Considerations&lt;/h3&gt;
&lt;p&gt;要提高深度神经网络的性能，最直接的方法就是增大规模。增大网络规模的方式有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增加网络深度（网络层数量）&lt;/li&gt;
&lt;li&gt;增加网络宽度（每层的神经元数量）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但直接增大网络规模的这种方法存在两个主要缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更大的规模意味着更多的参数，这会使得规模大的网络会更容易过拟合；&lt;/li&gt;
&lt;li&gt;更大的规模会造成计算资源的急剧增加，如果增加的部分的效率并不高，那么大量的计算资源都因此被浪费了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;解决这两个问题的最根本的方法就是将网络结构（包括卷积层内部）彻底从全连接（fully connected）变为稀疏连接（sparsely connected），对此有生物系统模拟和Arora等人的研究可佐证。但在实际应用中，将全连接变为稀疏连接后计算量并没有很大提升，这是因为现有硬件是针对密集矩阵进行计算优化的。由此提出Inception模块，在使用现有的计算稠密稀疏矩阵的硬件设备的条件下，利用稀疏连接提高网络的性能。&lt;/p&gt;
&lt;h3 id=&#34;architectural-details&#34;&gt;Architectural Details&lt;/h3&gt;
&lt;h4 id=&#34;inception-module&#34;&gt;Inception module&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet01.png&#34; alt=&#34;inception module a&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inception v1模块，将1x1，3x3，5x5 conv和3x3 pooling组成并联网络。一方面增加了网络的宽度，另一方面增加了网络对不同尺度的适应性（不同大小的卷积核支路所对应的感受野不同）。&lt;/p&gt;
&lt;h4 id=&#34;inception-module-with-dimension-reductions&#34;&gt;Inception module with dimension reductions&lt;/h4&gt;
&lt;p&gt;对于最初设计的Inception模块，虽然5x5的卷积核较少，但当网络达到一定规模后，仍然会产生巨大的计算量。为了解决这个问题，作者引入1x1的卷积核进行降维。对Inception v1的改进如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet03.png&#34; alt=&#34;inception module b&#34;&gt;&lt;/p&gt;
&lt;p&gt;改动后的结构有两个优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过降维，可以减少模型参数量；&lt;/li&gt;
&lt;li&gt;新增1x1卷积后可以带来更多的非线性变换，提高模型表达能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;
&lt;p&gt;GoogLeNet的网络细节如下表所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet02.png&#34; alt=&#34;GoogLeNet incarnation of the Inception architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;网络的一些特点如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络采用了模块化的结构，使用Inception mudule，便于修改网络结构。&lt;/li&gt;
&lt;li&gt;包括Inception在内的所有卷积都使用修正线性激活（ReLU）。&lt;/li&gt;
&lt;li&gt;网络使用average pooling代替了全连接层，但仍需要保留dropout。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;梯度回传&lt;/strong&gt;：为了避免梯度消失，网络中额外增加了2个辅助分类器（softmax）用于向前传播梯度。（辅助分类器只在训练时使用）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;将最佳稀疏结构稠密化是提高计算机视觉神经网络的有效方法。相比于浅且窄的网络，这种方法的优点在于只需适度增加计算量，性能就显著提升。&lt;/p&gt;
&lt;h2 id=&#34;part-2--inception系列&#34;&gt;Part 2 : Inception系列&lt;/h2&gt;
&lt;p&gt;论文：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.00567.pdf&#34;&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.07261.pdf&#34;&gt;Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;inception-v2&#34;&gt;Inception V2&lt;/h3&gt;
&lt;h4 id=&#34;4条设计原则&#34;&gt;4条设计原则&lt;/h4&gt;
&lt;p&gt;文章提出了4条设计原则，并依据这4条原则对Inception进行改进：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;网络浅层慎用bottleneck&lt;/li&gt;
&lt;li&gt;高维特征更适合在网络局部处理&lt;/li&gt;
&lt;li&gt;网络聚合可以通过低维嵌入&lt;/li&gt;
&lt;li&gt;平衡网络的深度和宽度&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;inception-v2-结构&#34;&gt;Inception V2 结构&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet04.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inception v2相对于v1做的改进是使用多个小卷积核代替一个大卷积核（如使用2个3x3的卷积代替原来的1个5x5卷积），这样可以有效减少模型的参数量，增加模型的深度。&lt;/p&gt;
&lt;p&gt;此外，Inception v2还引入了BN，加速网络训练，解决梯度消失。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet05.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;在此基础上，作者提出可以使用多个非对称的小尺寸卷积核堆叠，代替一个大卷积核（比如用1xn和nx1代替nxn）。需要注意的是，这种结构在前几层的效果并不好，当特征图的尺寸在12到20之间时使用的效果会更好一些。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet06.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;结合对称卷积和非对称卷积，增加网络宽度。&lt;/p&gt;
&lt;h3 id=&#34;inception-v3&#34;&gt;Inception V3&lt;/h3&gt;
&lt;p&gt;Inception v3中作者将7x7卷积分解成了3个3x3卷积，v3中使用的Aug loss里使用了BN进行regularization。&lt;/p&gt;
&lt;h4 id=&#34;降低特征图大小&#34;&gt;降低特征图大小&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet07.png&#34; alt=&#34;传统下采样&#34;&gt;&lt;/p&gt;
&lt;p&gt;传统的两种下采样方式如上图所示，要么先pooling再Inception（这种方法的缺点是池化会造成信息的丢失），要么先Inception再Pooling（这种方法的缺点是计算量增大）。故两种方法都不可取。作者提出了一种新的降低特征图大小的方法，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet08.png&#34; alt=&#34;inception下采样&#34;&gt;&lt;/p&gt;
&lt;p&gt;让池化和卷积并行执行（stride=2），最后再将特征图进行组合。&lt;/p&gt;
&lt;h3 id=&#34;inception-v4&#34;&gt;Inception V4&lt;/h3&gt;
&lt;h4 id=&#34;整体网络结构&#34;&gt;整体网络结构&lt;/h4&gt;
&lt;p&gt;相比于v2/v3，Inception v4的网络结构更加简介同意，并使用了更多的Inception模块。下图为Inception v4的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet09.png&#34; alt=&#34;inception v4整体网络结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;下图为Inception v4的stem模块。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet10.png&#34; alt=&#34;inception v4 stem module&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;inception-resnet&#34;&gt;Inception ResNet&lt;/h3&gt;
&lt;p&gt;Inception ResNet将Inception模块与残差连接思想结合，该系列有Inception-ResNet-v1和Inception-ResNet-v2，经验证，残差连接能够显著加速Inception的训练。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>VGG</title>
      <link>https://JuneCly.github.io/post/vgg/</link>
      <pubDate>Sat, 27 Aug 2022 13:36:02 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/vgg/</guid>
      
        <description>&lt;p&gt;VGG是由牛津大学视觉几何小组（&lt;strong&gt;V&lt;/strong&gt;isual &lt;strong&gt;G&lt;/strong&gt;eometry &lt;strong&gt;G&lt;/strong&gt;roup）提出的一种深层卷积网络，在2014年的ILSVRC比赛中获得了分类任务的亚军（同年该项冠军为GoogLeNet）和定位任务的冠军。&lt;/p&gt;
&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34;&gt;Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-vgg网络架构&#34;&gt;1. VGG网络架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG02.png&#34; alt=&#34;VGG convnet configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;VGG网络由5层卷积层，3层全连接层以及softmax输出层构成，层与层之间使用max pooling（最大池化）分开。作者根据具体卷积层的不同，共设计了6种网络结构，如上图所示，分别是A、A-LRN、B、C、D、E。这6种结构的网络深度从11层到19层。其中，D和E结构即是我们所熟知的VGG16和VGG19。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG01.jpg&#34; alt=&#34;VGG16&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图为VGG16的网络结构图。第1层卷积层由2个conv3-64组成，第2层卷积层由2个conv3-128组成，第3层卷积层由3个conv3-256组成，第4层卷积层由3个conv3-512组成，第5层卷积层由3个conv3-512组成，后接3个全连接层，前两个FC输出通道数为4096，后1个FC输出通道数为1000，最后经过softmax。共计16层。&lt;/p&gt;
&lt;h2 id=&#34;2-网络特点&#34;&gt;2. 网络特点&lt;/h2&gt;
&lt;h3 id=&#34;21-结构简单&#34;&gt;2.1 结构简单&lt;/h3&gt;
&lt;p&gt;作者在文中提出的6种网络结构虽在细节上有所不同，但VGG总体的网络结构保持一致，即5卷积+3全连接+softmax，并且层与层之间使用maxpool分开。所有隐层的激活函数均为ReLU。&lt;/p&gt;
&lt;h3 id=&#34;22-小卷积核&#34;&gt;2.2 小卷积核&lt;/h3&gt;
&lt;p&gt;VGG使用多个小卷积核（3x3）来代替一个较大的卷积层。例如，2个3x3的卷积层相当于1个5x5卷积，3个3x3卷积相当于1个7x7卷积。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG03.png&#34; alt=&#34;感受野&#34;&gt;&lt;/p&gt;
&lt;p&gt;使用小卷积核有两点好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少模型参数量&lt;/li&gt;
&lt;li&gt;因卷积核变小二导致的层数增加，可以增加模型的非线性表达能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23-小池化核&#34;&gt;2.3 小池化核&lt;/h3&gt;
&lt;p&gt;相比于AlexNet的3x3的池化核，VGG全部采用2x2的池化核。&lt;/p&gt;
&lt;h3 id=&#34;24-通道数更多&#34;&gt;2.4 通道数更多&lt;/h3&gt;
&lt;p&gt;更多的通道数可以表示更多的图像特征，VGG网络每个卷积层都对通道数进行了翻倍操作，直至增大到512个通道数。这样就可以提取出更多的信息。&lt;/p&gt;
&lt;h3 id=&#34;25-全连接转卷积&#34;&gt;2.5 全连接转卷积&lt;/h3&gt;
&lt;p&gt;VGG在&lt;strong&gt;测试阶段&lt;/strong&gt;将训练阶段的3个全连接层替换为3个卷积层，这样做的优点是可以处理任意大小尺寸的图片输入，并减少特征位置对分类的影响。（注：这个特点是作者参考了OverFeat的工作思路。）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG04.png&#34; alt=&#34;测试阶段全连接转卷积&#34;&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AlexNet</title>
      <link>https://JuneCly.github.io/post/alexnet/</link>
      <pubDate>Mon, 22 Aug 2022 15:57:49 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/alexnet/</guid>
      
        <description>&lt;p&gt;AlexNet在2012的ImageNet的竞赛中获得了冠军，该模型由Hintom和他的学生Alex Krizhevsky等人提出。 AlexNet由5个卷积层和3个全连接层构成，并首次使用ReLU、LRN、Dropout等技巧来提高网络的准确率。&lt;/p&gt;
&lt;p&gt;论文：&lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&#34;&gt;AlexNet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;alexnet架构&#34;&gt;AlexNet架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/AlexNet01.png&#34; alt=&#34;AlexNet架构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tricks&#34;&gt;Tricks&lt;/h2&gt;
&lt;p&gt;AlexNet应用的一些特殊的方法（按重要性进行排序）&lt;/p&gt;
&lt;h3 id=&#34;relu&#34;&gt;ReLU&lt;/h3&gt;
&lt;p&gt;AlexNet首次使用ReLU函数代替sigmoid和Tanh作为激活函数，这是因为相比于后两个函数，ReLU可以更好的解决梯度消失的问题。当ReLU函数的输入大于0时直接返回原值，输入小于0时则返回0。这样既可以利用正输入的梯度为1这个特点解决梯度消失问题，也可以利用负输入的输出为0这一特性，增加模型的稀疏性表示，进而加速并简化模型。（需要注意的是，ReLU常作为CNN的激活函数，但通常情况下并不适合作为RNN的激活函数）&lt;/p&gt;
&lt;h3 id=&#34;多gpu训练&#34;&gt;多GPU训练&lt;/h3&gt;
&lt;p&gt;由于GTX 580 GPU只有3GB的内存，因此将AlexNet网络分布在两块GPU上进行训练。需要额外注意的是，在训练时，GPU只在某些特定的层上进行通信，比如第3层的核会将第2层的所有核映射作为输入，第4层的核只将位于同一GPU上的第3层的核映射作为输入（这一点在网络架构图中可以看出）。&lt;/p&gt;
&lt;h3 id=&#34;lrn-局部响应归一化&#34;&gt;LRN (局部响应归一化)&lt;/h3&gt;
&lt;p&gt;局部响应归一化 (Local Response Normalization) 通过模拟生物医学中的“侧抑制”（被激活的神经元会抑制周围的神经元）来实现局部抑制，增强模型的泛化能力。（注：现在LRN已经逐渐被BN所取代）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;归一化的优点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加快收敛速度&lt;/li&gt;
&lt;li&gt;提高模型精度&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;重叠池化&#34;&gt;重叠池化&lt;/h3&gt;
&lt;p&gt;通常池化的窗口大小z与步长s相等，因此池化作用的区域不存在重叠的部分。但重叠池化令z&amp;gt;s，使得池化区域之间存在重叠的部分。AlexNet这篇论文指出，采用重叠池化的模型更不容易过拟合。&lt;/p&gt;
&lt;h2 id=&#34;减少过拟合&#34;&gt;减少过拟合&lt;/h2&gt;
&lt;p&gt;AlexNet主要使用两种方法来克服过拟合。&lt;/p&gt;
&lt;h3 id=&#34;数据增强&#34;&gt;数据增强&lt;/h3&gt;
&lt;p&gt;AlexNet用两种方式实现数据增强。因这两种方式都是由原始图像经非常少的计算量产生变换得到的图像，因此无需存储在硬盘上。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;随机裁剪、水平翻转&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改变训练图像的RGB通道的强度&lt;/strong&gt;：对RGB像素值集合执行PCA，并对主成分做一个标准差为0.1的高斯扰动。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;
&lt;p&gt;在训练过程中以p=0.5的概率对每个隐层神经元的输出设为0，以此丢弃部分神经元。这些被丢弃的神经元将不再进行前向传播并且不参与反向传播。采用droput方法可以避免过拟合。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Test</title>
      <link>https://JuneCly.github.io/post/test/</link>
      <pubDate>Mon, 15 Aug 2022 15:33:51 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/test/</guid>
      
        <description>&lt;p&gt;测试&lt;/p&gt;
&lt;h2 id=&#34;paragraph-and-line-breaks&#34;&gt;Paragraph and line breaks&lt;/h2&gt;
&lt;p&gt;A paragraph is simply one or more consecutive lines of text. In markdown source code, paragraphs are separated by more than one blank lines. In Typora, you only need to press &lt;code&gt;Return&lt;/code&gt; to create a new paragraph.&lt;/p&gt;
&lt;p&gt;Press &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Return&lt;/code&gt; to create a single line break. However, most markdown parser will ignore single line break, to make other markdown parsers recognize your line break, you can leave two whitespace at the end of the line, or insert &lt;code&gt;&amp;lt;br/&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;headers&#34;&gt;Headers&lt;/h2&gt;
&lt;p&gt;Headers use 1-6 hash characters at the start of the line, corresponding to header levels 1-6. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# This is an H1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## This is an H2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;###### This is an H6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In typora, input ‘#’s followed by title content, and press &lt;code&gt;Return&lt;/code&gt; key will create a header.&lt;/p&gt;
&lt;h2 id=&#34;blockquotes&#34;&gt;Blockquotes&lt;/h2&gt;
&lt;p&gt;Markdown uses email-style &amp;gt; characters for block quoting. They are presented as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a blockquote with two paragraphs. This is first paragraph.&lt;/p&gt;
&lt;p&gt;This is second pragraph.Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&lt;/p&gt;
&lt;p&gt;This is another blockquote with one paragraph. There is three empty line to seperate two blockquote.&lt;/p&gt;
&lt;p&gt;这是一段中文测试。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In typora, just input ‘&amp;gt;’ followed by quote contents a block quote is  generated. Typora will insert proper ‘&amp;gt;’ or line break for you. Block quote inside anther block quote is allowed by adding additional levels of ‘&amp;gt;’.&lt;/p&gt;
&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;Input &lt;code&gt;* list item 1&lt;/code&gt; will create an un-ordered list, the &lt;code&gt;*&lt;/code&gt; symbol can be replace with &lt;code&gt;+&lt;/code&gt; or &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Input &lt;code&gt;1. list item 1&lt;/code&gt; will create an ordered list, their markdown source code is like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;task-list&#34;&gt;Task List&lt;/h2&gt;
&lt;p&gt;Task lists are lists with items marked as either &lt;code&gt;[ ]&lt;/code&gt; or &lt;code&gt;[x]&lt;/code&gt; (incomplete or complete). For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; a task list item&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; list syntax required&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; normal &lt;strong&gt;formatting&lt;/strong&gt;, @mentions, #1234 refs&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; incomplete&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; completed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can change the complete/incomplete state by click the checkbox before the item.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
