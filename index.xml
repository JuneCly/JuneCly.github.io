<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chen LY</title>
    <link>https://JuneCly.github.io/</link>
    <description>Recent content on Chen LY</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 01 Sep 2022 10:57:09 +0800</lastBuildDate>
    
        <atom:link href="https://JuneCly.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GoogLeNet</title>
      <link>https://JuneCly.github.io/post/googlenet/</link>
      <pubDate>Thu, 01 Sep 2022 10:57:09 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/googlenet/</guid>
      
        <description>&lt;p&gt;GoogLeNet是google提出的基于Inception模块的串并联网络架构，并获得了2014年ILSVRC比赛的分类任务的冠军（同年该项亚军为VGG）。Inception模块及其迭代改进的版本可以提升模型的泛化能力、降低模型参数。&lt;/p&gt;
&lt;h2 id=&#34;part-1--goolenet&#34;&gt;Part 1 : GooLeNet&lt;/h2&gt;
&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34;&gt;Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;abstract--introduction&#34;&gt;Abstract &amp;amp; Introduction&lt;/h3&gt;
&lt;p&gt;GoogLeNet在ILSVRC14的分类和检测比赛中达到了新高度。这种网络结构提高了网络内计算资源的利用率，在保持计算量不变的同时增加网络的宽度和深度。GoogLeNet的网络架构思想基于Hebbian原则和多尺度处理的直觉，并且提出的Inception模块可以增加网络的深度。&lt;/p&gt;
&lt;h3 id=&#34;motivation-and-high-level-considerations&#34;&gt;Motivation and High Level Considerations&lt;/h3&gt;
&lt;p&gt;要提高深度神经网络的性能，最直接的方法就是增大规模。增大网络规模的方式有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增加网络深度（网络层数量）&lt;/li&gt;
&lt;li&gt;增加网络宽度（每层的神经元数量）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但直接增大网络规模的这种方法存在两个主要缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更大的规模意味着更多的参数，这会使得规模大的网络会更容易过拟合；&lt;/li&gt;
&lt;li&gt;更大的规模会造成计算资源的急剧增加，如果增加的部分的效率并不高，那么大量的计算资源都因此被浪费了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;解决这两个问题的最根本的方法就是将网络结构（包括卷积层内部）彻底从全连接（fully connected）变为稀疏连接（sparsely connected），对此有生物系统模拟和Arora等人的研究可佐证。但在实际应用中，将全连接变为稀疏连接后计算量并没有很大提升，这是因为现有硬件是针对密集矩阵进行计算优化的。由此提出Inception模块，在使用现有的计算稠密稀疏矩阵的硬件设备的条件下，利用稀疏连接提高网络的性能。&lt;/p&gt;
&lt;h3 id=&#34;architectural-details&#34;&gt;Architectural Details&lt;/h3&gt;
&lt;h4 id=&#34;inception-module&#34;&gt;Inception module&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet01.png&#34; alt=&#34;inception module a&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inception v1模块，将1x1，3x3，5x5 conv和3x3 pooling组成并联网络。一方面增加了网络的宽度，另一方面增加了网络对不同尺度的适应性（不同大小的卷积核支路所对应的感受野不同）。&lt;/p&gt;
&lt;h4 id=&#34;inception-module-with-dimension-reductions&#34;&gt;Inception module with dimension reductions&lt;/h4&gt;
&lt;p&gt;对于最初设计的Inception模块，虽然5x5的卷积核较少，但当网络达到一定规模后，仍然会产生巨大的计算量。为了解决这个问题，作者引入1x1的卷积核进行降维。对Inception v1的改进如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet03.png&#34; alt=&#34;inception module b&#34;&gt;&lt;/p&gt;
&lt;p&gt;改动后的结构有两个优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过降维，可以减少模型参数量；&lt;/li&gt;
&lt;li&gt;新增1x1卷积后可以带来更多的非线性变换，提高模型表达能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;
&lt;p&gt;GoogLeNet的网络细节如下表所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet02.png&#34; alt=&#34;GoogLeNet incarnation of the Inception architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;网络的一些特点如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络采用了模块化的结构，使用Inception mudule，便于修改网络结构。&lt;/li&gt;
&lt;li&gt;包括Inception在内的所有卷积都使用修正线性激活（ReLU）。&lt;/li&gt;
&lt;li&gt;网络使用average pooling代替了全连接层，但仍需要保留dropout。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;梯度回传&lt;/strong&gt;：为了避免梯度消失，网络中额外增加了2个辅助分类器（softmax）用于向前传播梯度。（辅助分类器只在训练时使用）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;将最佳稀疏结构稠密化是提高计算机视觉神经网络的有效方法。相比于浅且窄的网络，这种方法的优点在于只需适度增加计算量，性能就显著提升。&lt;/p&gt;
&lt;h2 id=&#34;part-2--inception系列&#34;&gt;Part 2 : Inception系列&lt;/h2&gt;
&lt;p&gt;论文：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.00567.pdf&#34;&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.07261.pdf&#34;&gt;Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;inception-v2&#34;&gt;Inception V2&lt;/h3&gt;
&lt;h4 id=&#34;4条设计原则&#34;&gt;4条设计原则&lt;/h4&gt;
&lt;p&gt;文章提出了4条设计原则，并依据这4条原则对Inception进行改进：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;网络浅层慎用bottleneck&lt;/li&gt;
&lt;li&gt;高维特征更适合在网络局部处理&lt;/li&gt;
&lt;li&gt;网络聚合可以通过低维嵌入&lt;/li&gt;
&lt;li&gt;平衡网络的深度和宽度&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;inception-v2-结构&#34;&gt;Inception V2 结构&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet04.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inception v2相对于v1做的改进是使用多个小卷积核代替一个大卷积核（如使用2个3x3的卷积代替原来的1个5x5卷积），这样可以有效减少模型的参数量，增加模型的深度。&lt;/p&gt;
&lt;p&gt;此外，Inception v2还引入了BN，加速网络训练，解决梯度消失。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet05.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;在此基础上，作者提出可以使用多个非对称的小尺寸卷积核堆叠，代替一个大卷积核（比如用1xn和nx1代替nxn）。需要注意的是，这种结构在前几层的效果并不好，当特征图的尺寸在12到20之间时使用的效果会更好一些。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet06.png&#34; alt=&#34;Inception v2&#34;&gt;&lt;/p&gt;
&lt;p&gt;结合对称卷积和非对称卷积，增加网络宽度。&lt;/p&gt;
&lt;h3 id=&#34;inception-v3&#34;&gt;Inception V3&lt;/h3&gt;
&lt;p&gt;Inception v3中作者将7x7卷积分解成了3个3x3卷积，v3中使用的Aug loss里使用了BN进行regularization。&lt;/p&gt;
&lt;h4 id=&#34;降低特征图大小&#34;&gt;降低特征图大小&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet07.png&#34; alt=&#34;传统下采样&#34;&gt;&lt;/p&gt;
&lt;p&gt;传统的两种下采样方式如上图所示，要么先pooling再Inception（这种方法的缺点是池化会造成信息的丢失），要么先Inception再Pooling（这种方法的缺点是计算量增大）。故两种方法都不可取。作者提出了一种新的降低特征图大小的方法，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet08.png&#34; alt=&#34;inception下采样&#34;&gt;&lt;/p&gt;
&lt;p&gt;让池化和卷积并行执行（stride=2），最后再将特征图进行组合。&lt;/p&gt;
&lt;h3 id=&#34;inception-v4&#34;&gt;Inception V4&lt;/h3&gt;
&lt;h4 id=&#34;整体网络结构&#34;&gt;整体网络结构&lt;/h4&gt;
&lt;p&gt;相比于v2/v3，Inception v4的网络结构更加简介同意，并使用了更多的Inception模块。下图为Inception v4的网络结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet09.png&#34; alt=&#34;inception v4整体网络结构&#34;&gt;&lt;/p&gt;
&lt;p&gt;下图为Inception v4的stem模块。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/GoogLeNet10.png&#34; alt=&#34;inception v4 stem module&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;inception-resnet&#34;&gt;Inception ResNet&lt;/h3&gt;
&lt;p&gt;Inception ResNet将Inception模块与残差连接思想结合，该系列有Inception-ResNet-v1和Inception-ResNet-v2，经验证，残差连接能够显著加速Inception的训练。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>VGG</title>
      <link>https://JuneCly.github.io/post/vgg/</link>
      <pubDate>Sat, 27 Aug 2022 13:36:02 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/vgg/</guid>
      
        <description>&lt;p&gt;VGG是由牛津大学视觉几何小组（&lt;strong&gt;V&lt;/strong&gt;isual &lt;strong&gt;G&lt;/strong&gt;eometry &lt;strong&gt;G&lt;/strong&gt;roup）提出的一种深层卷积网络，在2014年的ILSVRC比赛中获得了分类任务的亚军（同年该项冠军为GoogLeNet）和定位任务的冠军。&lt;/p&gt;
&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34;&gt;Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-vgg网络架构&#34;&gt;1. VGG网络架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG02.png&#34; alt=&#34;VGG convnet configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;VGG网络由5层卷积层，3层全连接层以及softmax输出层构成，层与层之间使用max pooling（最大池化）分开。作者根据具体卷积层的不同，共设计了6种网络结构，如上图所示，分别是A、A-LRN、B、C、D、E。这6种结构的网络深度从11层到19层。其中，D和E结构即是我们所熟知的VGG16和VGG19。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG01.jpg&#34; alt=&#34;VGG16&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图为VGG16的网络结构图。第1层卷积层由2个conv3-64组成，第2层卷积层由2个conv3-128组成，第3层卷积层由3个conv3-256组成，第4层卷积层由3个conv3-512组成，第5层卷积层由3个conv3-512组成，后接3个全连接层，前两个FC输出通道数为4096，后1个FC输出通道数为1000，最后经过softmax。共计16层。&lt;/p&gt;
&lt;h2 id=&#34;2-网络特点&#34;&gt;2. 网络特点&lt;/h2&gt;
&lt;h3 id=&#34;21-结构简单&#34;&gt;2.1 结构简单&lt;/h3&gt;
&lt;p&gt;作者在文中提出的6种网络结构虽在细节上有所不同，但VGG总体的网络结构保持一致，即5卷积+3全连接+softmax，并且层与层之间使用maxpool分开。所有隐层的激活函数均为ReLU。&lt;/p&gt;
&lt;h3 id=&#34;22-小卷积核&#34;&gt;2.2 小卷积核&lt;/h3&gt;
&lt;p&gt;VGG使用多个小卷积核（3x3）来代替一个较大的卷积层。例如，2个3x3的卷积层相当于1个5x5卷积，3个3x3卷积相当于1个7x7卷积。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG03.png&#34; alt=&#34;感受野&#34;&gt;&lt;/p&gt;
&lt;p&gt;使用小卷积核有两点好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少模型参数量&lt;/li&gt;
&lt;li&gt;因卷积核变小二导致的层数增加，可以增加模型的非线性表达能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23-小池化核&#34;&gt;2.3 小池化核&lt;/h3&gt;
&lt;p&gt;相比于AlexNet的3x3的池化核，VGG全部采用2x2的池化核。&lt;/p&gt;
&lt;h3 id=&#34;24-通道数更多&#34;&gt;2.4 通道数更多&lt;/h3&gt;
&lt;p&gt;更多的通道数可以表示更多的图像特征，VGG网络每个卷积层都对通道数进行了翻倍操作，直至增大到512个通道数。这样就可以提取出更多的信息。&lt;/p&gt;
&lt;h3 id=&#34;25-全连接转卷积&#34;&gt;2.5 全连接转卷积&lt;/h3&gt;
&lt;p&gt;VGG在&lt;strong&gt;测试阶段&lt;/strong&gt;将训练阶段的3个全连接层替换为3个卷积层，这样做的优点是可以处理任意大小尺寸的图片输入，并减少特征位置对分类的影响。（注：这个特点是作者参考了OverFeat的工作思路。）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/VGG04.png&#34; alt=&#34;测试阶段全连接转卷积&#34;&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>AlexNet</title>
      <link>https://JuneCly.github.io/post/alexnet/</link>
      <pubDate>Mon, 22 Aug 2022 15:57:49 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/alexnet/</guid>
      
        <description>&lt;p&gt;AlexNet在2012的ImageNet的竞赛中获得了冠军，该模型由Hintom和他的学生Alex Krizhevsky等人提出。 AlexNet由5个卷积层和3个全连接层构成，并首次使用ReLU、LRN、Dropout等技巧来提高网络的准确率。&lt;/p&gt;
&lt;p&gt;论文：&lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&#34;&gt;AlexNet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;alexnet架构&#34;&gt;AlexNet架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://JuneCly.github.io/image/AlexNet01.png&#34; alt=&#34;AlexNet架构&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tricks&#34;&gt;Tricks&lt;/h2&gt;
&lt;p&gt;AlexNet应用的一些特殊的方法（按重要性进行排序）&lt;/p&gt;
&lt;h3 id=&#34;relu&#34;&gt;ReLU&lt;/h3&gt;
&lt;p&gt;AlexNet首次使用ReLU函数代替sigmoid和Tanh作为激活函数，这是因为相比于后两个函数，ReLU可以更好的解决梯度消失的问题。当ReLU函数的输入大于0时直接返回原值，输入小于0时则返回0。这样既可以利用正输入的梯度为1这个特点解决梯度消失问题，也可以利用负输入的输出为0这一特性，增加模型的稀疏性表示，进而加速并简化模型。（需要注意的是，ReLU常作为CNN的激活函数，但通常情况下并不适合作为RNN的激活函数）&lt;/p&gt;
&lt;h3 id=&#34;多gpu训练&#34;&gt;多GPU训练&lt;/h3&gt;
&lt;p&gt;由于GTX 580 GPU只有3GB的内存，因此将AlexNet网络分布在两块GPU上进行训练。需要额外注意的是，在训练时，GPU只在某些特定的层上进行通信，比如第3层的核会将第2层的所有核映射作为输入，第4层的核只将位于同一GPU上的第3层的核映射作为输入（这一点在网络架构图中可以看出）。&lt;/p&gt;
&lt;h3 id=&#34;lrn-局部响应归一化&#34;&gt;LRN (局部响应归一化)&lt;/h3&gt;
&lt;p&gt;局部响应归一化 (Local Response Normalization) 通过模拟生物医学中的“侧抑制”（被激活的神经元会抑制周围的神经元）来实现局部抑制，增强模型的泛化能力。（注：现在LRN已经逐渐被BN所取代）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;归一化的优点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加快收敛速度&lt;/li&gt;
&lt;li&gt;提高模型精度&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;重叠池化&#34;&gt;重叠池化&lt;/h3&gt;
&lt;p&gt;通常池化的窗口大小z与步长s相等，因此池化作用的区域不存在重叠的部分。但重叠池化令z&amp;gt;s，使得池化区域之间存在重叠的部分。AlexNet这篇论文指出，采用重叠池化的模型更不容易过拟合。&lt;/p&gt;
&lt;h2 id=&#34;减少过拟合&#34;&gt;减少过拟合&lt;/h2&gt;
&lt;p&gt;AlexNet主要使用两种方法来克服过拟合。&lt;/p&gt;
&lt;h3 id=&#34;数据增强&#34;&gt;数据增强&lt;/h3&gt;
&lt;p&gt;AlexNet用两种方式实现数据增强。因这两种方式都是由原始图像经非常少的计算量产生变换得到的图像，因此无需存储在硬盘上。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;随机裁剪、水平翻转&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改变训练图像的RGB通道的强度&lt;/strong&gt;：对RGB像素值集合执行PCA，并对主成分做一个标准差为0.1的高斯扰动。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;
&lt;p&gt;在训练过程中以p=0.5的概率对每个隐层神经元的输出设为0，以此丢弃部分神经元。这些被丢弃的神经元将不再进行前向传播并且不参与反向传播。采用droput方法可以避免过拟合。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Test</title>
      <link>https://JuneCly.github.io/post/test/</link>
      <pubDate>Mon, 15 Aug 2022 15:33:51 +0800</pubDate>
      
      <guid>https://JuneCly.github.io/post/test/</guid>
      
        <description>&lt;p&gt;测试&lt;/p&gt;
&lt;h2 id=&#34;paragraph-and-line-breaks&#34;&gt;Paragraph and line breaks&lt;/h2&gt;
&lt;p&gt;A paragraph is simply one or more consecutive lines of text. In markdown source code, paragraphs are separated by more than one blank lines. In Typora, you only need to press &lt;code&gt;Return&lt;/code&gt; to create a new paragraph.&lt;/p&gt;
&lt;p&gt;Press &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Return&lt;/code&gt; to create a single line break. However, most markdown parser will ignore single line break, to make other markdown parsers recognize your line break, you can leave two whitespace at the end of the line, or insert &lt;code&gt;&amp;lt;br/&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;headers&#34;&gt;Headers&lt;/h2&gt;
&lt;p&gt;Headers use 1-6 hash characters at the start of the line, corresponding to header levels 1-6. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# This is an H1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## This is an H2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;###### This is an H6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In typora, input ‘#’s followed by title content, and press &lt;code&gt;Return&lt;/code&gt; key will create a header.&lt;/p&gt;
&lt;h2 id=&#34;blockquotes&#34;&gt;Blockquotes&lt;/h2&gt;
&lt;p&gt;Markdown uses email-style &amp;gt; characters for block quoting. They are presented as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a blockquote with two paragraphs. This is first paragraph.&lt;/p&gt;
&lt;p&gt;This is second pragraph.Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.&lt;/p&gt;
&lt;p&gt;This is another blockquote with one paragraph. There is three empty line to seperate two blockquote.&lt;/p&gt;
&lt;p&gt;这是一段中文测试。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In typora, just input ‘&amp;gt;’ followed by quote contents a block quote is  generated. Typora will insert proper ‘&amp;gt;’ or line break for you. Block quote inside anther block quote is allowed by adding additional levels of ‘&amp;gt;’.&lt;/p&gt;
&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;Input &lt;code&gt;* list item 1&lt;/code&gt; will create an un-ordered list, the &lt;code&gt;*&lt;/code&gt; symbol can be replace with &lt;code&gt;+&lt;/code&gt; or &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Input &lt;code&gt;1. list item 1&lt;/code&gt; will create an ordered list, their markdown source code is like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Red&lt;/li&gt;
&lt;li&gt;Green&lt;/li&gt;
&lt;li&gt;Blue&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;task-list&#34;&gt;Task List&lt;/h2&gt;
&lt;p&gt;Task lists are lists with items marked as either &lt;code&gt;[ ]&lt;/code&gt; or &lt;code&gt;[x]&lt;/code&gt; (incomplete or complete). For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; a task list item&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; list syntax required&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; normal &lt;strong&gt;formatting&lt;/strong&gt;, @mentions, #1234 refs&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; incomplete&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; completed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can change the complete/incomplete state by click the checkbox before the item.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
